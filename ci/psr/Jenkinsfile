// Copyright (c) 2022, Oracle and/or its affiliates.
// Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

def DOCKER_IMAGE_TAG
def SUSPECT_LIST = ""
def VERRAZZANO_DEV_VERSION = ""
def EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS = false
def tarfilePrefix=""
def storeLocation=""

def agentLabel = env.JOB_NAME.contains('master') ? "phx-large" : "large"

pipeline {
    options {
        skipDefaultCheckout true
        copyArtifactPermission('*');
        timestamps ()
    }

    agent {
       docker {
            image "${RUNNER_DOCKER_IMAGE}"
            args "${RUNNER_DOCKER_ARGS}"
            registryUrl "${RUNNER_DOCKER_REGISTRY_URL}"
            registryCredentialsId 'ocir-pull-and-push-account'
            label "${agentLabel}"
        }
    }

    parameters {
        choice (name: 'KUBERNETES_CLUSTER_VERSION', description: 'Kubernetes Version for KinD Cluster', choices: [ "1.24", "1.23", "1.22", "1.21" ])
        string (name: 'VZ_BRANCH_TO_USE', defaultValue: 'CURRENT', trim: true, description: 'This is the name of the Verrazzano product branch to use for testing the PSR tooling; CURRENT means using the working branch')
        string (name: 'VERRAZZANO_OPERATOR_IMAGE', defaultValue: 'NONE', trim: true, description: 'Uses a specific Verrazzano platform operator image name (in ghcr.io repo).  If not specified, the latest operator.yaml from related Verrazzano repo branch will be used to create Verrazzano platform operator manifest')
        string (name: 'KIND_NODE_COUNT', defaultValue: '3', trim: true, description: 'Number of nodes for the KIND cluster for smoke testing')
        booleanParam (description: 'Whether to create the cluster with Calico for AT testing (defaults to true)', name: 'CREATE_CLUSTER_USE_CALICO', defaultValue: true)
        booleanParam (description: 'Whether to capture full cluster snapshot on test failure', name: 'CAPTURE_FULL_CLUSTER', defaultValue: false)
        booleanParam (description: 'Whether to dump k8s cluster on success (off by default can be useful to capture for comparing to failed cluster)', name: 'DUMP_K8S_CLUSTER_ON_SUCCESS', defaultValue: false)

        booleanParam (name: 'PERFORM_SCAN', defaultValue: false, description: 'Whether to perform a scan of the built images')
        booleanParam (name: 'EMIT_METRICS', defaultValue: false, description: 'Whether to emit metrics from the pipeline')
        booleanParam (name: 'FAIL_IF_COVERAGE_DECREASED', defaultValue: false, description: 'Whether to fail build if UT coverage number decreases lower than its release-* coverage from object storage. This defaults to true, meaning Any non release-*/master branch will fail if its coverage is lower. This can be disabled so that jobs only WARN when coverage drops but not fail.')
        booleanParam (name: 'UPLOAD_UNIT_TEST_COVERAGE', defaultValue: false, description: 'Whether to write the UT coverage number to object storage. This always occurs for release-*/master branches. Defaults to true, but it can be disabled to not always upload.')
        booleanParam (name: 'SKIP_BUILD', defaultValue: false, description: "Skip the PSR tool and Worker builds")
        booleanParam (name: 'VZ_TEST_DEBUG', defaultValue: true, description: "Enables debug of Verrazzano installation scripts")
        string (name: 'TAGGED_TESTS', defaultValue: '', trim: true, description: 'A comma separated list of build tags for tests that should be executed (e.g. unstable_test). Default:')
        string (name: 'INCLUDED_TESTS', defaultValue: '.*', description: 'A regex matching any fully qualified test file that should be executed (e.g. examples/helidon/). Default: .*', trim: true)
        string (name: 'EXCLUDED_TESTS', defaultValue: '_excluded_test', description: 'A regex matching any fully qualified test file that should not be executed (e.g. multicluster/|_excluded_test). Default: _excluded_test', trim: true)
    }

    environment {
        TEST_ENV = "PSR"

        DOCKER_PSR_BACKEND_PUBLISH_IMAGE_NAME = 'psr-backend'
        DOCKER_PSR_BRANCH_BACKEND_IMAGE_NAME = '${DOCKER_PSR_BACKEND_PUBLISH_IMAGE_NAME}-jenkins'
        DOCKER_PSR_BACKEND_IMAGE_NAME = "${env.BRANCH_NAME ==~ /^release-.*/ || env.BRANCH_NAME == 'master' ? env.DOCKER_PSR_BACKEND_PUBLISH_IMAGE_NAME : env.DOCKER_PSR_BRANCH_BACKEND_IMAGE_NAME}"

        GOPATH = '/home/opc/go'
        GO_REPO_PATH = "${GOPATH}/src/github.com/verrazzano"
        VZ_ROOT="${GO_REPO_PATH}/verrazzano"
        PSR_PATH = "${VZ_ROOT}/tools/psr"

        DOCKER_CREDS = credentials('github-packages-credentials-rw')
        DOCKER_EMAIL = credentials('github-packages-email')
        DOCKER_REGISTRY = 'ghcr.io'
        DOCKER_REPO = "verrazzano"
        OCR_CREDS = credentials('ocr-pull-and-push-account')
        OCR_REPO = 'container-registry.oracle.com'

        NETRC_FILE = credentials('netrc')
        GITHUB_PKGS_CREDS = credentials('github-packages-credentials-rw')
        SERVICE_KEY = credentials('PAGERDUTY_SERVICE_KEY')

        // used for console artifact capture on failure
        JENKINS_READ = credentials('jenkins-auditor')

        OCI_CLI_AUTH="instance_principal"
        OCI_OS_NAMESPACE = credentials('oci-os-namespace')
        OCI_OS_ARTIFACT_BUCKET="build-failure-artifacts"
        OCI_OS_BUCKET="verrazzano-builds"
        OCI_OS_COMMIT_BUCKET="verrazzano-builds-by-commit"
        OCI_OS_REGION="us-phoenix-1"

        // used to emit metrics
        PROMETHEUS_GW_URL = credentials('prometheus-dev-url')
        PROMETHEUS_CREDENTIALS = credentials('prometheus-credentials')

        OCIR_SCAN_COMPARTMENT = credentials('ocir-scan-compartment')
        OCIR_SCAN_TARGET = credentials('ocir-scan-target')
        OCIR_SCAN_REGISTRY = credentials('ocir-scan-registry')
        OCIR_SCAN_REPOSITORY_PATH = credentials('ocir-scan-repository-path')
        DOCKER_SCAN_CREDS = credentials('v8odev-ocir')

        //VZ_TEST_BRANCH = "${params.VZ_BRANCH_TO_USE}"

        // Environment variable for Verrazzano CLI executable
        VZ_COMMAND="${WORKSPACE}/vz"

        // used to generate Ginkgo test reports
        TEST_REPORT = "test-report.xml"
        GINKGO_REPORT_ARGS = "--junit-report=${TEST_REPORT} --keep-separate-reports=true"
        TEST_REPORT_DIR = "${WORKSPACE}/tests/e2e"
        TESTS_EXECUTED_FILE = "${WORKSPACE}/tests_executed_file.tmp"

        KUBECONFIG = "${WORKSPACE}/test_kubeconfig"
        VERRAZZANO_KUBECONFIG = "${KUBECONFIG}"
        WILDCARD_DNS_DOMAIN = "nip.io"

        PSR_COMMAND = "${VZ_ROOT}/psrctl"
    }

    stages {
        stage('Clean workspace and checkout') {
            steps {
                sh """
                    echo "${NODE_LABELS}"
                """

                script {
                    def scmInfo = checkout scm
                    env.GIT_COMMIT = scmInfo.GIT_COMMIT
                    env.GIT_BRANCH = scmInfo.GIT_BRANCH
                    echo "SCM checkout of ${env.GIT_BRANCH} at ${env.GIT_COMMIT}"
                }
                sh """
                    cp -f "${NETRC_FILE}" $HOME/.netrc
                    chmod 600 $HOME/.netrc
                """

                script {
                    try {
                        sh """
                            echo "${DOCKER_CREDS_PSW}" | docker login ${env.DOCKER_REGISTRY} -u ${DOCKER_CREDS_USR} --password-stdin
                        """
                    } catch(error) {
                        echo "docker login failed, retrying after sleep"
                        retry(4) {
                            sleep(30)
                            sh """
                            echo "${DOCKER_CREDS_PSW}" | docker login ${env.DOCKER_REGISTRY} -u ${DOCKER_CREDS_USR} --password-stdin
                            """
                        }
                    }
                }
                moveContentToGoRepoPath()

                script {
                    EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS = getEffectiveDumpOnSuccess()
                    // Create the image tag
                    def props = readProperties file: '.verrazzano-development-version'
                    VERRAZZANO_DEV_VERSION = props['verrazzano-development-version']
                    TIMESTAMP = sh(returnStdout: true, script: "date +%Y%m%d%H%M%S").trim()
                    SHORT_COMMIT_HASH = sh(returnStdout: true, script: "echo $env.GIT_COMMIT | head -c 8")
                    env.VERRAZZANO_VERSION = "${VERRAZZANO_DEV_VERSION}"
                    if (!"${env.GIT_BRANCH}".startsWith("release-")) {
                        env.VERRAZZANO_VERSION = "${env.VERRAZZANO_VERSION}-${env.BUILD_NUMBER}+${SHORT_COMMIT_HASH}"
                    }
                    DOCKER_IMAGE_TAG = "v${VERRAZZANO_DEV_VERSION}-${TIMESTAMP}-${SHORT_COMMIT_HASH}"

                    env.VZ_TEST_BRANCH = params.VZ_BRANCH_TO_USE
                    if (params.VZ_BRANCH_TO_USE == "CURRENT") {
                        env.VZ_TEST_BRANCH = env.BRANCH_NAME
                    }
                    echo "VZ_TEST_BRANCH: ${env.VZ_TEST_BRANCH}"

                    // update the description with some meaningful info
                    currentBuild.description = SHORT_COMMIT_HASH + " : " + env.GIT_COMMIT
                    def currentCommitHash = env.GIT_COMMIT
                    def commitList = getCommitList()
                    withCredentials([file(credentialsId: 'jenkins-to-slack-users', variable: 'JENKINS_TO_SLACK_JSON')]) {
                        def userMappings = readJSON file: JENKINS_TO_SLACK_JSON
                        SUSPECT_LIST = getSuspectList(commitList, userMappings)
                        echo "Suspect list: ${SUSPECT_LIST}"
                    }
                }
            }
        }

        stage('Parallel Build, Test, and Compliance') {
            parallel {
                stage('Build PSR CLI and Save Binaries') {
                    when {
                        expression {params.SKIP_BUILD == false}
                    }
                    environment {
                      DOCKER_REPO="${DOCKER_REPO}"
                      DOCKER_REGISTRY="${DOCKER_REGISTRY}"
                      DOCKER_IMAGE_NAME="${DOCKER_PSR_BACKEND_IMAGE_NAME}"
                      DOCKER_IMAGE_TAG="${DOCKER_IMAGE_TAG}"
                    }
                    steps {
                        buildCLI()
                    }
                    post {
                        success {
                            script {
                                archiveArtifacts artifacts: '**/*.tar.gz*', allowEmptyArchive: true
                                echo "Saving CLI Binary"
                                saveCLIExecutables()
                            }

                        }
                    }
                }

                stage('Build PSR Images and Save Generated Files') {
                    when {
                       allOf {
                          not { buildingTag() }
                          expression {params.SKIP_BUILD == false}
                       }
                    }
                    environment {
                      DOCKER_REPO="${DOCKER_REPO}"
                      DOCKER_REGISTRY="${DOCKER_REGISTRY}"
                      DOCKER_IMAGE_NAME="${DOCKER_PSR_BACKEND_IMAGE_NAME}"
                      DOCKER_IMAGE_TAG="${DOCKER_IMAGE_TAG}"
                    }
                    steps {
                        script {
                            VZ_BUILD_METRIC = metricJobName('psrbuild')
                            metricTimerStart("${VZ_BUILD_METRIC}")
                            buildImages("${DOCKER_IMAGE_TAG}")
                        }
                    }
                    post {
                        failure {
                            script {
                                METRICS_PUSHED=metricTimerEnd("${VZ_BUILD_METRIC}", '0')
                            }
                        }
                        success {
                            echo "Saving generated files"
                            script {
                                METRICS_PUSHED=metricTimerEnd("${VZ_BUILD_METRIC}", '1')
                            }
                        }
                    }
                }

                stage('Quality, Compliance Checks, and Unit Tests') {
                    when {
                       allOf {
                          not { buildingTag() }
                          expression {params.SKIP_BUILD == false}
                       }
                    }
                    steps {
                        sh """
                            cd ${PSR_PATH}
                            make psr-quality
                        """
                    }
                    post {
                        always {
                            sh """
                                cd ${PSR_PATH}
                                cp coverage.html ${WORKSPACE}
                                cp coverage.xml ${WORKSPACE}
                                ${VZ_ROOT}/build/copy-junit-output.sh ${WORKSPACE}
                             """
                            cobertura(coberturaReportFile: 'coverage.xml',
                                    enableNewApi: true,
                                    autoUpdateHealth: false,
                                    autoUpdateStability: false,
                                    failUnstable: true,
                                    failUnhealthy: true,
                                    failNoReports: true,
                                    onlyStable: false,
                                    fileCoverageTargets: '100, 0, 0',
                                    lineCoverageTargets: '68, 68, 68',
                                    packageCoverageTargets: '100, 0, 0',
                            )
                            archiveArtifacts artifacts: '**/coverage.html,**/logs/**', allowEmptyArchive: true
                            junit testResults: '**/*test-result.xml', allowEmptyResults: true
                        }
                    }
                }
            }
        }

        stage('Scan Images') {
            when {
               allOf {
                  not { buildingTag() }
                  expression {params.PERFORM_SCAN == true}
               }
            }
            steps {
                script {
                    scanContainerImage "${env.DOCKER_REGISTRY}/${env.DOCKER_REPO}/${DOCKER_PSR_BACKEND_IMAGE_NAME}:${DOCKER_IMAGE_TAG}"
                }
            }
            post {
                always {
                    archiveArtifacts artifacts: '**/scanning-report*.json', allowEmptyArchive: true
               }
            }
        }

        stage('Scenario Tests') {
            environment {
                DOCKER_REPO="${env.DOCKER_REGISTRY}"
                KIND_KUBERNETES_CLUSTER_VERSION="${params.KUBERNETES_CLUSTER_VERSION}"
                OCI_OS_BUCKET="${env.OCI_OS_BUCKET}"
                OCI_OS_LOCATION="${env.VZ_TEST_BRANCH}"
                KIND_NODE_COUNT="${env.KIND_NODE_COUNT}"
                PSR_COMMAND="${env.PSR_COMMAND}"
                CREATE_CLUSTER_USE_CALICO="${params.CREATE_CLUSTER_USE_CALICO}"
            }
            steps {
                script {
                    runMakeCommand("all")
                }
            }
            post {
                success {
                    script {
                        VZ_TEST_METRIC = metricJobName('psr-smoke-test')
                        METRICS_PUSHED=metricTimerEnd("${VZ_TEST_METRIC}", '1')
                        if (EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS == true) {
                            dumpK8sCluster('psr-tests-success-cluster-snapshot')
                        }
                    }
                }
                failure {
                    script {
                        VZ_TEST_METRIC = metricJobName('psr-smoke-test')
                        METRICS_PUSHED=metricTimerEnd("${VZ_TEST_METRIC}", '0')
                        if ( fileExists(env.TESTS_EXECUTED_FILE) ) {
                            dumpK8sCluster('psr-tests-failure-cluster-snapshot')
                        }
                        postFailureProcessing()
                    }
                }
                cleanup {
                    runMakeCommand("cleanup")
                }
            }
        }
    }

    post {
        always {
            archiveArtifacts artifacts: "**/scanning-report*.json,**/coverage.html,**/logs/**,**/verrazzano_images.txt,**/*full-cluster*/**,**/bug-report/**,**/Screenshot*.png,**/ConsoleLog*.log,**/${TEST_REPORT}", allowEmptyArchive: true
            junit testResults: '**/*test-result.xml', allowEmptyResults: true
        }
        failure {
            sh """
                curl -k -u ${JENKINS_READ_USR}:${JENKINS_READ_PSW} -o archive.zip ${BUILD_URL}artifact/*zip*/archive.zip
                oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_ARTIFACT_BUCKET} --name ${env.JOB_NAME}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}/archive.zip --file archive.zip
                rm archive.zip
            """
            script {
                if (isPagerDutyEnabled() && (env.JOB_NAME == "verrazzano/master" || env.JOB_NAME ==~ "verrazzano/release-1.*")) {
                    pagerduty(resolve: false, serviceKey: "$SERVICE_KEY", incDescription: "Verrazzano: ${env.JOB_NAME} - Failed", incDetails: "Job Failed - \"${env.JOB_NAME}\" build: ${env.BUILD_NUMBER}\n\nView the log at:\n ${env.BUILD_URL}\n\nBlue Ocean:\n${env.RUN_DISPLAY_URL}")
                }
                if (env.JOB_NAME == "verrazzano/master" || env.JOB_NAME ==~ "verrazzano/release-1.*" || env.BRANCH_NAME ==~ "mark/*") {
                    slackSend ( channel: "$SLACK_ALERT_CHANNEL", message: "Job Failed - \"${env.JOB_NAME}\" build: ${env.BUILD_NUMBER}\n\nView the log at:\n ${env.BUILD_URL}\n\nBlue Ocean:\n${env.RUN_DISPLAY_URL}\n\nSuspects:\n${SUSPECT_LIST}" )
                }
            }
        }
        cleanup {
            metricBuildDuration()
            deleteDir()
        }
    }
}

def runMakeCommand(makeTarget) {
    sh """
       cd ${VZ_ROOT}/ci/psr
       make -I ${VZ_ROOT}/ci/make ${makeTarget}
    """
}

// Called in Stage CLI steps
def buildCLI() {
    sh """
        cd ${PSR_PATH}
        make go-build-cli
        ${VZ_ROOT}/ci/scripts/save_psr_tooling.sh ${env.BRANCH_NAME} ${SHORT_COMMIT_HASH}
        cp out/linux_amd64/psrctl ${PSR_COMMAND}
    """
}

// Called in Stage Build steps
// Makes target docker push for psr-backend image
def buildImages(dockerImageTag) {
    sh """
        cd ${PSR_PATH}
        echo 'Building PSR backend image...'
        make docker-push DOCKER_REGISTRY=${env.DOCKER_REGISTRY} DOCKER_REPO=${env.DOCKER_REPO} DOCKER_IMAGE_NAME=${env.DOCKER_PSR_BACKEND_IMAGE_NAME} DOCKER_IMAGE_TAG=${dockerImageTag}
    """
}

// REVIEW: seems redundant with the save_psr_tooling script?
def saveCLIExecutables() {
    sh """
        cd ${VZ_ROOT}/tools/psr
        oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name ${env.BRANCH_NAME}/psrctl-linux-amd64.tar.gz --file $WORKSPACE/psrctl-linux-amd64.tar.gz
        oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/${env.BRANCH_NAME}/${SHORT_COMMIT_HASH}/psrctl-linux-amd64.tar.gz --file $WORKSPACE/psrctl-linux-amd64.tar.gz
        oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name ${env.BRANCH_NAME}/psrctl-linux-amd64.tar.gz.sha256 --file $WORKSPACE/psrctl-linux-amd64.tar.gz.sha256
        oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/${env.BRANCH_NAME}/${SHORT_COMMIT_HASH}/psrctl-linux-amd64.tar.gz.sha256 --file $WORKSPACE/psrctl-linux-amd64.tar.gz.sha256
    """
}

// Called in many parallel stages of Stage Run Acceptance Tests
def runGinkgoRandomize(testSuitePath) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        sh """
            cd ${VZ_ROOT}/tests/e2e
            if [ -d "${testSuitePath}" ]; then
                ginkgo -p --randomize-all -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/...
            fi
            ../../build/copy-junit-output.sh ${WORKSPACE}
        """
    }
}

// Called in many parallel stages of Stage Run Acceptance Tests
def runGinkgo(testSuitePath) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        sh """
            cd ${VZ_ROOT}/tests/e2e
            ginkgo -v -keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/...
            ../../build/copy-junit-output.sh ${WORKSPACE}
        """
    }
}

def dumpK8sCluster(dumpDirectory) {
    sh """
        ${VZ_ROOT}/ci/scripts/capture_cluster_snapshot.sh ${dumpDirectory}
    """
}

def postFailureProcessing() {
    sh """
        curl -k -u ${JENKINS_READ_USR}:${JENKINS_READ_PSW} -o ${WORKSPACE}/build-console-output.log ${BUILD_URL}consoleText
    """
    archiveArtifacts artifacts: '**/build-console-output.log', allowEmptyArchive: true
    sh """
        curl -k -u ${JENKINS_READ_USR}:${JENKINS_READ_PSW} -o archive.zip ${BUILD_URL}artifact/*zip*/archive.zip
        oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_ARTIFACT_BUCKET} --name ${env.JOB_NAME}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}/archive.zip --file archive.zip
        rm archive.zip
    """
    script {
        if (env.BRANCH_NAME == "master" || env.BRANCH_NAME ==~ "release-.*" || env.BRANCH_NAME ==~ "mark/*") {
            slackSend ( message: "Job Failed - \"${env.JOB_NAME}\" build: ${env.BUILD_NUMBER}\n\nView the log at:\n ${env.BUILD_URL}\n\nBlue Ocean:\n${env.RUN_DISPLAY_URL}" )
        }
    }
}

def getEffectiveDumpOnSuccess() {
    def effectiveValue = params.DUMP_K8S_CLUSTER_ON_SUCCESS
    if (FORCE_DUMP_K8S_CLUSTER_ON_SUCCESS.equals("true") && (env.BRANCH_NAME.equals("master"))) {
        effectiveValue = true
        echo "Forcing dump on success based on global override setting"
    }
    return effectiveValue
}

def isPagerDutyEnabled() {
    // this controls whether PD alerts are enabled
    //if (NOTIFY_PAGERDUTY_MAINJOB_FAILURES.equals("true")) {
    //    echo "Pager-Duty notifications enabled via global override setting"
    //    return true
    //}
    return false
}

// Called in Stage Clean workspace and checkout steps
def moveContentToGoRepoPath() {
    sh """
        rm -rf ${VZ_ROOT}
        mkdir -p ${VZ_ROOT}
        tar cf - . | (cd ${VZ_ROOT}/ ; tar xf -)
    """
}

// Called in Stage Clean workspace and checkout steps
@NonCPS
def getCommitList() {
    echo "Checking for change sets"
    def commitList = []
    def changeSets = currentBuild.changeSets
    for (int i = 0; i < changeSets.size(); i++) {
        echo "get commits from change set"
        def commits = changeSets[i].items
        for (int j = 0; j < commits.length; j++) {
            def commit = commits[j]
            def id = commit.commitId
            echo "Add commit id: ${id}"
            commitList.add(id)
        }
    }
    return commitList
}

def trimIfGithubNoreplyUser(userIn) {
    if (userIn == null) {
        echo "Not a github noreply user, not trimming: ${userIn}"
        return userIn
    }
    if (userIn.matches(".*\\+.*@users.noreply.github.com.*")) {
        def userOut = userIn.substring(userIn.indexOf("+") + 1, userIn.indexOf("@"))
        return userOut;
    }
    if (userIn.matches(".*<.*@users.noreply.github.com.*")) {
        def userOut = userIn.substring(userIn.indexOf("<") + 1, userIn.indexOf("@"))
        return userOut;
    }
    if (userIn.matches(".*@users.noreply.github.com")) {
        def userOut = userIn.substring(0, userIn.indexOf("@"))
        return userOut;
    }
    echo "Not a github noreply user, not trimming: ${userIn}"
    return userIn
}

def getSuspectList(commitList, userMappings) {
    def retValue = ""
    def suspectList = []
    if (commitList == null || commitList.size() == 0) {
        echo "No commits to form suspect list"
    } else {
        for (int i = 0; i < commitList.size(); i++) {
            def id = commitList[i]
            try {
                def gitAuthor = sh(
                    script: "git log --format='%ae' '$id^!'",
                    returnStdout: true
                ).trim()
                if (gitAuthor != null) {
                    def author = trimIfGithubNoreplyUser(gitAuthor)
                    echo "DEBUG: author: ${gitAuthor}, ${author}, id: ${id}"
                    if (userMappings.containsKey(author)) {
                        def slackUser = userMappings.get(author)
                        if (!suspectList.contains(slackUser)) {
                            echo "Added ${slackUser} as suspect"
                            retValue += " ${slackUser}"
                            suspectList.add(slackUser)
                        }
                    } else {
                        // If we don't have a name mapping use the commit.author, at least we can easily tell if the mapping gets dated
                        if (!suspectList.contains(author)) {
                            echo "Added ${author} as suspect"
                            retValue += " ${author}"
                            suspectList.add(author)
                        }
                    }
                } else {
                    echo "No author returned from git"
                }
            } catch (Exception e) {
                echo "INFO: Problem processing commit ${id}, skipping commit: " + e.toString()
            }
        }
    }
    def startedByUser = "";
    def causes = currentBuild.getBuildCauses()
    echo "causes: " + causes.toString()
    for (cause in causes) {
        def causeString = cause.toString()
        echo "current cause: " + causeString
        def causeInfo = readJSON text: causeString
        if (causeInfo.userId != null) {
            startedByUser = causeInfo.userId
        }
    }

    if (startedByUser.length() > 0) {
        echo "Build was started by a user, adding them to the suspect notification list: ${startedByUser}"
        def author = trimIfGithubNoreplyUser(startedByUser)
        echo "DEBUG: author: ${startedByUser}, ${author}"
        if (userMappings.containsKey(author)) {
            def slackUser = userMappings.get(author)
            if (!suspectList.contains(slackUser)) {
                echo "Added ${slackUser} as suspect"
                retValue += " ${slackUser}"
                suspectList.add(slackUser)
            }
        } else {
            // If we don't have a name mapping use the commit.author, at least we can easily tell if the mapping gets dated
            if (!suspectList.contains(author)) {
               echo "Added ${author} as suspect"
               retValue += " ${author}"
               suspectList.add(author)
            }
        }
    } else {
        echo "Build not started by a user, not adding to notification list"
    }
    echo "returning suspect list: ${retValue}"
    return retValue
}

def metricJobName(stageName) {
    job = env.JOB_NAME.split("/")[0]
    job = '_' + job.replaceAll('-','_')
    if (stageName) {
        job = job + '_' + stageName
    }
    return job
}

def metricTimerStart(metricName) {
    def timerStartName = "${metricName}_START"
    env."${timerStartName}" = sh(returnStdout: true, script: "date +%s").trim()
}

// Construct the set of labels/dimensions for the metrics
def getMetricLabels() {
    def buildNumber = String.format("%010d", env.BUILD_NUMBER.toInteger())
    labels = 'build_number=\\"' + "${buildNumber}"+'\\",' +
             'jenkins_build_number=\\"' + "${env.BUILD_NUMBER}"+'\\",' +
             'jenkins_job=\\"' + "${env.JOB_NAME}".replace("%2F","/") + '\\",' +
             'commit_sha=\\"' + "${env.GIT_COMMIT}"+'\\"'
    return labels
}

def metricTimerEnd(metricName, status) {
    def timerStartName = "${metricName}_START"
    def timerEndName   = "${metricName}_END"
    env."${timerEndName}" = sh(returnStdout: true, script: "date +%s").trim()
    if (params.EMIT_METRICS) {
        long x = env."${timerStartName}" as long;
        long y = env."${timerEndName}" as long;
        def dur = (y-x)
        labels = getMetricLabels()
        withCredentials([usernameColonPassword(credentialsId: 'prometheus-credentials', variable: 'PROMETHEUS_CREDENTIALS')]) {
            EMIT = sh(returnStdout: true, script: "ci/scripts/metric_emit.sh ${PROMETHEUS_GW_URL} ${PROMETHEUS_CREDENTIALS} ${metricName} ${env.GIT_BRANCH} $labels ${status} ${dur}")
            echo "emit prometheus metrics: $EMIT"
            return EMIT
        }
    } else {
        return ''
    }
}

// Emit the metrics indicating the duration and result of the build
def metricBuildDuration() {
    def status = "${currentBuild.currentResult}".trim()
    long duration = "${currentBuild.duration}" as long;
    long durationInSec = (duration/1000)
    testMetric = metricJobName('')
    def metricValue = "-1"
    statusLabel = status.substring(0,1)
    if (status.equals("SUCCESS")) {
        metricValue = "1"
    } else if (status.equals("FAILURE")) {
        metricValue = "0"
    } else {
        // Consider every other status as a single label
        statusLabel = "A"
    }
    if (params.EMIT_METRICS) {
        labels = getMetricLabels()
        labels = labels + ',result=\\"' + "${statusLabel}"+'\\"'
        withCredentials([usernameColonPassword(credentialsId: 'prometheus-credentials', variable: 'PROMETHEUS_CREDENTIALS')]) {
            METRIC_STATUS = sh(returnStdout: true, returnStatus: true, script: "ci/scripts/metric_emit.sh ${PROMETHEUS_GW_URL} ${PROMETHEUS_CREDENTIALS} ${testMetric}_job ${env.BRANCH_NAME} $labels ${metricValue} ${durationInSec}")
            echo "Publishing the metrics for build duration and status returned status code $METRIC_STATUS"
        }
    }
}

// Copyright (c) 2023, Oracle and/or its affiliates.
// Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.

def DOCKER_IMAGE_TAG
def SKIP_ACCEPTANCE_TESTS = false
def EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS = false
def availableRegions = [  "us-ashburn-1", "ca-montreal-1", "ca-toronto-1", "eu-amsterdam-1", "eu-frankfurt-1", "eu-zurich-1", "uk-london-1" ]
def acmeEnvironments = [ "staging", "production" ]
Collections.shuffle(availableRegions)
def zoneId = UUID.randomUUID().toString().substring(0,6).replace('-','')
def dns_zone_ocid = 'dummy'
def OKE_CLUSTER_PREFIX = ""
def agentLabel = env.JOB_NAME.contains('master') ? "phx-large" : "large"

def KUBECTL = "kubectl"
def VZ_CLI = "vz cli"
def installationMethodsPreupgrade = [ KUBECTL, VZ_CLI ]
Collections.shuffle(installationMethodsPreupgrade)
def installationMethodsUpgrade = [ KUBECTL, VZ_CLI ]
Collections.shuffle(installationMethodsUpgrade)

installerFileName = "install-verrazzano.yaml"

pipeline {
    options {
        skipDefaultCheckout true
        timestamps ()
    }

    agent {
        docker {
            image "${RUNNER_DOCKER_IMAGE}"
            args "${RUNNER_DOCKER_ARGS}"
            registryUrl "${RUNNER_DOCKER_REGISTRY_URL}"
            registryCredentialsId 'ocir-pull-and-push-account'
            label "${agentLabel}"
        }
    }
    parameters {
        booleanParam (description: 'Whether to use External Elasticsearch', name: 'EXTERNAL_ELASTICSEARCH', defaultValue: false)
        choice (description: 'Number of Cluster', name: 'TOTAL_CLUSTERS', choices: ["2", "1", "3"])
        choice (description: 'Predefined config permutations for Verrazzano installation. Prod profile is the default profile for NONE', name: 'VZ_INSTALL_CONFIG',
                choices: ["NONE", "dev-kind-persistence"])
        choice (description: 'Verrazzano Test Environment', name: 'TEST_ENV',
                choices: ["KIND", "magicdns_oke", "ocidns_oke"])
        choice (description: 'ACME Certificate Environment (Staging or Production)', name: 'ACME_ENVIRONMENT',
                choices: acmeEnvironments)
        choice (description: 'OCI region to launch OKE clusters', name: 'OKE_CLUSTER_REGION',
                // 1st choice is the default value
                choices: availableRegions )
        choice (description: 'Verrazzano pre-upgrade installation using either kubectl or vz cli ', name: 'INSTALLATION_METHOD_PREUPGRADE',
                // 1st choice is the default value
                choices: installationMethodsPreupgrade )
        choice (description: 'Verrazzano upgrade installation using either kubectl or vz cli ', name: 'INSTALLATION_METHOD_UPGRADE',
                // 1st choice is the default value
                choices: installationMethodsUpgrade )
        choice (description: 'OKE node pool configuration', name: 'OKE_NODE_POOL',
                // 1st choice is the default value
                choices: [ "VM.Standard.E3.Flex-4-2", "VM.Standard2.4-2", "VM.Standard.E3.Flex-8-2" ])
        choice (name: 'OKE_CLUSTER_VERSION',
                description: 'Kubernetes Version for OKE Cluster',
                // 1st choice is the default value
                choices: [ "v1.24.1", "v1.25.4", "v1.26.2" ])
        choice (name: 'KIND_CLUSTER_VERSION',
                description: 'Kubernetes Version for KIND Cluster',
                // 1st choice is the default value
                choices: [ "1.24", "1.25", "1.26" ])
        string (name: 'VERSION_FOR_INSTALL',
                defaultValue: 'v1.4.0',
                description: 'This is the Verrazzano version for install before doing an upgrade.  By default, the v1.3.0 release will be installed',
                trim: true)
        string (name: 'VERSION_FOR_UPGRADE',
                defaultValue: 'NONE',
                description: 'This is the Verrazzano version for Upgrade. By default, the Verrazzano tip will be used.',
                trim: true)
        choice (name: 'IS_TRIGGERED_MANUALLY',
                description: 'This flag is used to test upgrade within existing releases. the default value is set to YES to enable users to trigger customized version upgrades. Default: YES',
                choices: [ "YES", "NO" ])
        string (name: 'GIT_COMMIT_TO_USE',
                defaultValue: 'NONE',
                description: 'This is the full git commit hash from the source build to be used for all jobs',
                trim: true)
        string (name: 'VERRAZZANO_OPERATOR_IMAGE',
                defaultValue: 'NONE',
                description: 'Verrazzano platform operator image name (in ghcr.io repo).  If not specified, the latest operator.yaml published to the Verrazzano Object Store will be used',
                trim: true)
        choice (name: 'ADMIN_CLUSTER_PROFILE',
                description: 'Verrazzano Admin Cluster install profile name',
                // 1st choice is the default value
                choices: [ "prod", "dev" ])
        choice (name: 'MANAGED_CLUSTER_PROFILE',
                description: 'Verrazzano Managed Cluster install profile name',
                // 1st choice is the default value
                choices: [ "managed-cluster", "prod", "dev" ])
        choice (name: 'WILDCARD_DNS_DOMAIN',
                description: 'This is the wildcard DNS domain',
                // 1st choice is the default value
                choices: [ "nip.io", "sslip.io"])
        choice (name: 'CRD_API_VERSION',
                description: 'This is the API crd version.',
                // 1st choice is the default value
                choices: [ "v1alpha1", "v1beta1"])
        booleanParam (description: 'Whether to create the cluster with Calico for AT testing', name: 'CREATE_CLUSTER_USE_CALICO', defaultValue: true)
        string (name: 'CONSOLE_REPO_BRANCH',
                defaultValue: '',
                description: 'The branch to check out after cloning the console repository.',
                trim: true)
        string (name: 'TAGGED_TESTS',
                defaultValue: '',
                description: 'A comma separated list of build tags for tests that should be executed (e.g. unstable_test). Default:',
                trim: true)
        string (name: 'INCLUDED_TESTS',
                defaultValue: '.*',
                description: 'A regex matching any fully qualified test file that should be executed (e.g. examples/helidon/). Default: .*',
                trim: true)
        string (name: 'EXCLUDED_TESTS',
                defaultValue: '_excluded_test',
                description: 'A regex matching any fully qualified test file that should not be executed (e.g. multicluster/|_excluded_test). Default: _excluded_test',
                trim: true)
    }

    environment {
        DOCKER_PLATFORM_CI_IMAGE_NAME = 'verrazzano-platform-operator-jenkins'
        DOCKER_PLATFORM_PUBLISH_IMAGE_NAME = 'verrazzano-platform-operator'
        DOCKER_PLATFORM_IMAGE_NAME = "${env.BRANCH_NAME == 'master' ? env.DOCKER_PLATFORM_PUBLISH_IMAGE_NAME : env.DOCKER_PLATFORM_CI_IMAGE_NAME}"
        DOCKER_OAM_CI_IMAGE_NAME = 'verrazzano-application-operator-jenkins'
        DOCKER_OAM_PUBLISH_IMAGE_NAME = 'verrazzano-application-operator'
        DOCKER_OAM_IMAGE_NAME = "${env.BRANCH_NAME == 'master' ? env.DOCKER_OAM_PUBLISH_IMAGE_NAME : env.DOCKER_OAM_CI_IMAGE_NAME}"
        CREATE_LATEST_TAG = "${env.BRANCH_NAME == 'master' ? '1' : '0'}"
        GOPATH = '/home/opc/go'
        GO_REPO_PATH = "${GOPATH}/src/github.com/verrazzano"
        DOCKER_CREDS = credentials('github-packages-credentials-rw')
        DOCKER_EMAIL = credentials('github-packages-email')
        DOCKER_REPO = 'ghcr.io'
        DOCKER_NAMESPACE = 'verrazzano'
        NETRC_FILE = credentials('netrc')
        CLUSTER_NAME_PREFIX = 'verrazzano'
        TESTS_EXECUTED_FILE = "${WORKSPACE}/tests_executed_file.tmp"
        POST_DUMP_FAILED_FILE = "${WORKSPACE}/post_dump_failed_file.tmp"
        KUBECONFIG_DIR = "${WORKSPACE}/kubeconfig"

        OCR_CREDS = credentials('ocr-pull-and-push-account')
        OCR_REPO = 'container-registry.oracle.com'
        IMAGE_PULL_SECRET = 'verrazzano-container-registry'

        TEST_ENV = "${params.TEST_ENV}"
        MANAGED_CLUSTER_PROFILE = "${params.MANAGED_CLUSTER_PROFILE}"
        ADMIN_CLUSTER_PROFILE = "${params.ADMIN_CLUSTER_PROFILE}"

        // Find a better way to handle this
        // OKE_CLUSTER_VERSION = "${params.KUBERNETES_VERSION == '1.17' ? 'v1.17.13' : 'v1.18.10'}"
        TF_VAR_compartment_id = credentials('oci-tiburon-dev-compartment-ocid')
        TF_VAR_tenancy_id = credentials('oci-tenancy')
        TF_VAR_tenancy_name = credentials('oci-tenancy-name')
        TF_VAR_user_id = credentials('oci-user-ocid')
        TF_VAR_region = "${params.OKE_CLUSTER_REGION}"
        TF_VAR_kubernetes_version = "${params.OKE_CLUSTER_VERSION}"
        TF_VAR_nodepool_config = "${params.OKE_NODE_POOL}"
        TF_VAR_api_fingerprint = credentials('oci-api-key-fingerprint')
        TF_VAR_api_private_key_path = credentials('oci-api-key')
        TF_VAR_s3_bucket_access_key = credentials('oci-s3-bucket-access-key')
        TF_VAR_s3_bucket_secret_key = credentials('oci-s3-bucket-secret-key')
        TF_VAR_ssh_public_key_path = credentials('oci-tf-pub-ssh-key')

        OCI_CLI_TENANCY = credentials('oci-tenancy')
        OCI_CLI_USER = credentials('oci-user-ocid')
        OCI_CLI_FINGERPRINT = credentials('oci-api-key-fingerprint')
        OCI_CLI_KEY_FILE = credentials('oci-api-key')
        OCI_CLI_REGION = "${params.OKE_CLUSTER_REGION}"
        OCI_CLI_SUPPRESS_FILE_PERMISSIONS_WARNING = 'True'

        INSTALL_CONFIG_FILE_KIND = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1alpha1/install-vz-prod-kind-upgrade.yaml"
        INSTALL_CONFIG_FILE_OCIDNS = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1alpha1/install-verrazzano-ocidns.yaml"
        INSTALL_CONFIG_FILE_NIPIO = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1alpha1/install-verrazzano-nipio.yaml"
        OCI_DNS_ZONE_NAME="z${zoneId}.v8o.io"
        ACME_ENVIRONMENT="${params.ACME_ENVIRONMENT}"
        CREATE_EXTERNAL_OPENSEARCH = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/create-external-os.sh"

        TIMESTAMP = sh(returnStdout: true, script: "date +%Y%m%d%H%M%S").trim()
        SHORT_TIME_STAMP = sh(returnStdout: true, script: "date +%m%d%H%M%S").trim()
        TEST_SCRIPTS_DIR = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts"
        LOOPING_TEST_SCRIPTS_DIR = "${TEST_SCRIPTS_DIR}/looping-test"
        ADMIN_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"

        // Environment variables required to capture cluster snapshot and bug report on test failure
        DUMP_COMMAND="${GO_REPO_PATH}/verrazzano/tools/scripts/k8s-dump-cluster.sh"
        TEST_DUMP_ROOT="${WORKSPACE}/test-cluster-snapshots"
        CAPTURE_FULL_CLUSTER="false"

        // Environment variable for Verrazzano CLI executable
        VZ_COMMAND="${GO_REPO_PATH}/vz"

        VERRAZZANO_INSTALL_LOGS_DIR="${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs"
        VERRAZZANO_INSTALL_LOG="verrazzano-install.log"
        EXTERNAL_ELASTICSEARCH = "${params.EXTERNAL_ELASTICSEARCH}"

        // used for console artifact capture on failure
        JENKINS_READ = credentials('jenkins-auditor')
        OCI_CLI_AUTH="instance_principal"
        OCI_OS_NAMESPACE = credentials('oci-os-namespace')
        OCI_OS_ARTIFACT_BUCKET="build-failure-artifacts"
        OCI_OS_BUCKET="verrazzano-builds"
        OCI_OS_COMMIT_BUCKET="verrazzano-builds-by-commit"
        OCI_OS_LOCATION="${OCI_OS_LOCATION}"
        VZ_CLI_TARGZ="vz-linux-amd64.tar.gz"

        // used to emit metrics
        PROMETHEUS_CREDENTIALS = credentials('prometheus-credentials')
        TEST_ENV_LABEL = "${params.TEST_ENV}"
        SEARCH_HTTP_ENDPOINT = credentials('search-gw-url')
        SEARCH_PASSWORD = "${PROMETHEUS_CREDENTIALS_PSW}"
        SEARCH_USERNAME = "${PROMETHEUS_CREDENTIALS_USR}"

        // sample app deployed before upgrade and UI console tests
        SAMPLE_APP_NAME="hello-helidon"
        SAMPLE_APP_NAMESPACE="hello-helidon"
        SAMPLE_APP_PROJECT="hello-helidon-sample-proj"
        SAMPLE_APP_COMPONENT="hello-helidon-component"

        // used to generate Ginkgo test reports
        TEST_REPORT = "test-report.xml"
        GINKGO_REPORT_ARGS = "--junit-report=${TEST_REPORT} --keep-separate-reports=true"
        TEST_REPORT_DIR = "${WORKSPACE}/tests/e2e"

        VERSION_FOR_UPGRADE_ENV = "${params.VERSION_FOR_UPGRADE}"
        VERSION_FOR_INSTALL = "${params.VERSION_FOR_INSTALL}"
        TARGET_UPGRADE_OPERATOR_YAML = "${WORKSPACE}/target-upgrade-operator.yaml"
        RUNNING_OPERATOR_VERSION = ""
    }

    stages {
        stage('Clean workspace and checkout') {
            steps {
                printNodeLabels()

                performSourceCodeCheckout()

                performNetRCOperations()

                performDockerLogin()

                performVerrazzanoDirectoryOperations()

                setInitValues()

                downloadCLI()
            }
        }

        stage('Install and Configure') {
            when {
                allOf {
                    not { buildingTag() }
                    anyOf {
                        branch 'master';
                        expression {SKIP_ACCEPTANCE_TESTS == false};
                    }
                }
            }
            stages {
                stage('Prepare AT environment') {
                    parallel {
                        stage('Create Kind Clusters') {
                            when { expression { return params.TEST_ENV == 'KIND' } }
                            steps {
                                createKindClusters()
                            }
                        }
                        stage('Create OKE Clusters') {
                            when { expression { return params.TEST_ENV == 'ocidns_oke' || params.TEST_ENV == 'magicdns_oke'} }
                            steps {
                                echo "OKE Cluster Prefix: ${OKE_CLUSTER_PREFIX}"
                                createOKEClusters("${OKE_CLUSTER_PREFIX}")
                            }
                        }
                    }
                }
                stage("Configure Clusters") {
                    parallel {
                        stage("Configure OKE/OCI DNS") {
                            when { expression { return params.TEST_ENV == 'ocidns_oke' } }
                            stages {
                                stage('Create OCI DNS zone') {
                                    steps {
                                        createOCIDNSZone()
                                    }
                                }
                                stage('Configure OCI DNS Resources') {
                                    environment {
                                        OCI_DNS_COMPARTMENT_OCID = credentials('oci-dns-compartment')
                                        OCI_PRIVATE_KEY_FILE = credentials('oci-api-key')
                                        OCI_DNS_ZONE_OCID = "${dns_zone_ocid}"
                                    }
                                    steps {
                                        createTestOCIConfigSecret()
                                    }
                                }
                                stage('Configure OCI DNS Installers') {
                                    environment {
                                        OCI_DNS_COMPARTMENT_OCID = credentials('oci-dns-compartment')
                                        OCI_PRIVATE_KEY_FILE = credentials('oci-api-key')
                                        OCI_DNS_ZONE_OCID = "${dns_zone_ocid}"
                                    }
                                    steps {
                                        script {
                                            configureVerrazzanoInstallers(env.INSTALL_CONFIG_FILE_OCIDNS, "./tests/e2e/config/scripts/process_oci_dns_install_yaml.sh", "acme", params.ACME_ENVIRONMENT)
                                        }
                                    }
                                }
                            }
                        }
                        stage("Configure KinD") {
                            when { expression { return params.TEST_ENV == 'KIND' } }
                            steps {
                                configureVerrazzanoInstallers(INSTALL_CONFIG_FILE_KIND,"./tests/e2e/config/scripts/process_kind_install_yaml.sh", params.WILDCARD_DNS_DOMAIN)
                            }
                        }
                        stage("Configure OKE/MagicDNS") {
                            when { expression { return params.TEST_ENV == 'magicdns_oke' } }
                            steps {
                                configureVerrazzanoInstallers(env.INSTALL_CONFIG_FILE_NIPIO, "./tests/e2e/config/scripts/process_nipio_install_yaml.sh", params.WILDCARD_DNS_DOMAIN)
                            }
                        }
                    }
                }
                stage ('Install Verrazzano') {
                    steps {
                        script {
                            getVerrazzanoOperatorYaml()
                        }
                        installVerrazzano()
                    }
                    post {
                        always {
                            script {
                                dumpInstallLogs()
                            }
                        }
                        failure {
                            script {
                                dumpK8sCluster("${WORKSPACE}/install-failure-cluster-snapshot")
                            }
                        }
                    }
                }
                stage('Multicluster Registration') {
                    when { expression { isMultiCluster() } }
                    stages {
                        stage('Register managed clusters') {
                            steps {
                                registerManagedClusters()
                            }
                        }
                        stage('verify-register') {
                            steps {
                                verifyRegisterManagedClusters(true)
                            }
                        }
                    }
                }
                stage ('Pre-upgrade Acceptance Tests') {
                    parallel {
                        stage('pre-upgrade verify keycloak') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/pre-upgrade/keycloak', "${TEST_DUMP_ROOT}/keycloak")
                            }
                        }
                        stage('opensearch pre-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/pre-upgrade/opensearch', "${TEST_DUMP_ROOT}/opensearch")
                            }
                        }
                        stage('opensearch-dashboards pre-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/pre-upgrade/opensearch-dashboards', "${TEST_DUMP_ROOT}/opensearch-dashboards")
                            }
                        }
                        stage('Grafana pre-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/pre-upgrade/grafana', "${TEST_DUMP_ROOT}/grafana")
                            }
                        }
                        stage('Metricsbinding pre-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/pre-upgrade/metricsbinding', "${TEST_DUMP_ROOT}/metricsbinding")
                            }
                        }
                        stage ('examples helidon') {
                            when { expression { isVZOperatorMinimumCompatibleVersion("v1.1.0") } }
                            steps {
                                runGinkgoVerify('examples/helidon', "${TEST_DUMP_ROOT}/helidon-workload", "false", "true", "true", "hello-helidon-sc")
                            }
                        }
                        stage ('mc examples helidon') {
                            when { expression { isMultiCluster() && isVZOperatorMinimumCompatibleVersion("v1.1.0") } }
                            steps {
                                runGinkgoVerifyManaged("multicluster/examples/helidon", "${TEST_DUMP_ROOT}/mchelidon-workload", "false", "true", "true")
                            }
                        }
                        stage ('coherence workload') {
                            when { expression { isVZOperatorMinimumCompatibleVersion("v1.1.0") } }
                            steps {
                                runGinkgoVerify('workloads/coherence', "${TEST_DUMP_ROOT}/coherence-workload", "false", "false", "true", "hello-coherence-sc")
                            }
                        }
                        stage ('mc coherence workload') {
                            when { expression { isMultiCluster() && isVZOperatorMinimumCompatibleVersion("v1.1.0") && checkUpgradeBelowVer("v1.3.0") } }
                            steps {
                                runGinkgoVerifyManaged("multicluster/workloads/mccoherence", "${TEST_DUMP_ROOT}/mccoherence-workload", "false", "true", "true")
                            }
                        }
                        stage('deployment metrics') {
                            steps {
                                runGinkgoVerifyParallel('Prometheus pre-upgrade', 'metrics/deploymetrics', "${TEST_DUMP_ROOT}/prompreupgrade", "false", "true", "true", "deploymetrics")
                            }
                        }
                    }
                }
                stage('upgrade-platform-operator using kubectl') {
                    when { expression { return params.INSTALLATION_METHOD_UPGRADE == "kubectl" } }
                    stages {
                        stage("upgrade-platform-operator") {
                            steps {
                                upgradePlatformOperator()
                            }
                        }
                        stage("Verify Upgrade Required") {
                            steps {
                                runGinkgoRandomizeSerial('upgrade/pre-upgrade/verify-upgrade-required')
                            }
                        }
                    }
                }
                stage("upgrade-verrazzano") {
                    steps {
                        upgradeVerrazzano()
                    }
                    post {
                        always {
                            performDescribeVerrazzanoResource()
                        }
                    }
                }
            }
            post {
                failure {
                    script {
                        dumpK8sCluster("${WORKSPACE}/multicluster-install-cluster-snapshot")
                    }
                }
            }
        }

        stage('Verify Upgrade') {
            // Rerun some stages to verify the upgrade
            parallel {
                stage ('verify-register') {
                    when { expression { isMultiCluster() } }
                    steps {
                        verifyRegisterManagedClusters(false)
                    }
                }
                stage ('verify-permissions') {
                    when { expression { isMultiCluster() } }
                    steps {
                        verifyManagedClusterPermissions()
                    }
                }
                stage ('system component metrics') {
                    steps {
                        runGinkgoRandomizeParallel('metrics/syscomponents')
                    }
                }
            }
            post {
                failure {
                    script {
                        dumpK8sCluster("${WORKSPACE}/multicluster-verify-upgrade-cluster-snapshot")
                    }
                }
            }
        }
        stage ('Verify Install') {
            parallel {
                stage('verify-install') {
                    steps {
                        runGinkgoRandomizeSerial('verify-install')
                    }
                }

                stage ('mc verify-install') {
                    when { expression { isMultiCluster() } }
                    steps {
                        runGinkgoRandomizeParallel('multicluster/verify-install')
                    }
                }
            }
        }

        // Run Verify Registry
        stage('verify-registry') {
            steps {
                catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
                    script {
                        runMulticlusterVerifyRegistry()
                    }
                }
            }
            post {
                always {
                    archiveArtifacts artifacts: '**/coverage.html,**/logs/*', allowEmptyArchive: true
                    junit testResults: '**/*test-result.xml', allowEmptyResults: true
                }
            }
        }

        stage ('Verify Infra') {
            parallel {
                stage('verify-scripts') {
                    steps {
                        runGinkgoRandomizeParallel('scripts/install')
                    }
                }
                stage('verify-infra restapi') {
                    steps {
                        runGinkgoRandomizeParallel('verify-infra/restapi')
                    }
                }
                stage('verify-infra oam') {
                    steps {
                        runGinkgoRandomizeParallel('verify-infra/oam')
                    }
                }
                stage('verify-infra vmi') {
                    steps {
                        runGinkgoRandomizeParallel('verify-infra/vmi')
                    }
                }
            }
            post {
                always {
                    archiveArtifacts artifacts: '**/coverage.html,**/logs/*', allowEmptyArchive: true
                    junit testResults: '**/*test-result.xml', allowEmptyResults: true
                }
            }
        }
        stage('Post-upgrade Acceptance Tests') {
            stages {
                stage ('Console') {
                    when { expression { isMultiCluster() } }
                        steps {
                            runConsoleTests()
                        }
                        post {
                            always {
                                saveConsoleTestArtifacts()
                            }
                        }
                }
                stage('Post-upgrade Acceptance Tests parallel') {
                    parallel {
                        stage('post-upgrade verify keycloak') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/post-upgrade/keycloak', "${TEST_DUMP_ROOT}/keycloak")
                            }
                        }
                        stage('opensearch post-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/post-upgrade/opensearch', "${TEST_DUMP_ROOT}/opensearch")
                            }
                        }
                        stage('Grafana post-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/post-upgrade/grafana', "${TEST_DUMP_ROOT}/grafana")
                            }
                        }
                        stage('Metricsbinding post-upgrade') {
                            steps {
                                runGinkgoRandomizeSerialAdmin('upgrade/post-upgrade/metricsbinding', "${TEST_DUMP_ROOT}/metricsbinding")
                            }
                        }
                        stage('deployment metrics') {
                            steps {
                                runGinkgoVerifyParallel('Prometheus post-upgrade', 'metrics/deploymetrics', "${TEST_DUMP_ROOT}/prompostupgrade", "true", "false", "false", "deploymetrics")
                            }
                        }
                        stage ("Verify install scripts") {
                            steps {
                                runGinkgoRandomizeAdmin("scripts/install", "${TEST_DUMP_ROOT}/install-scripts")
                            }
                        }
                        stage ('mc verify-api') {
                            when { expression { isMultiCluster() } }
                            steps {
                                catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
                                    script {
                                        runMulticlusterVerifyApi()
                                    }
                                }
                            }
                            post {
                                failure {
                                    script {
                                        dumpK8sCluster("${WORKSPACE}/multicluster-post-upgrade-acceptance-tests-cluster-snapshot-pre-uninstall")
                                    }
                                }
                            }
                        }
                    }
                }
                stage ('Verify & Uninstall Example Apps') {
                    when { expression { isMultiCluster() } }
                    parallel {
                        stage ('mc examples helidon') {
                            steps {
                                runGinkgoVerifyManaged("multicluster/examples/helidon", "${TEST_DUMP_ROOT}/mchelidon-workload", "true", "false", "false")
                            }
                        }
                        stage ('mc coherence workload') {
                            steps {
                                runGinkgoVerifyManaged("multicluster/workloads/mccoherence", "${TEST_DUMP_ROOT}/mccoherence-workload", checkUpgradeBelowVer("v1.3.0"), "false", "false")
                            }
                        }
                    }
                }
            }
            post {
                failure {
                    script {
                        dumpK8sCluster("${WORKSPACE}/multicluster-post-upgrade-acceptance-tests-cluster-snapshot")
                    }
                }
                aborted {
                    script {
                        dumpK8sCluster("${WORKSPACE}/multicluster-post-upgrade-acceptance-tests-cluster-snapshot")
                    }
                }
                success {
                    script {
                        if (EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS == true) {
                            dumpK8sCluster("${WORKSPACE}/multicluster-post-upgrade-acceptance-tests-cluster-snapshot")
                        }
                    }
                }
            }
        }
        stage('Cleanup Tests') {
            stages {
                stage('verify deregister') {
                    when { expression { isMultiCluster() } }
                    steps {
                        verifyDeregisterManagedClusters()
                    }
                }
            }
            post {
                failure {
                    script {
                        dumpK8sCluster("${WORKSPACE}/multicluster-cleanup-tests-cluster-snapshot")
                    }
                }
            }
        }
    }
    post {
        failure {
            pipelinePostFailure()
        }
        always {
            script {
                if ( fileExists(env.TESTS_EXECUTED_FILE) ) {
                    dumpAll()
                }
            }
            copyGeneratedTestReports()
            archiveArtifacts artifacts: "**/*-operator.yaml,**/install-verrazzano.yaml,**/kube_config,**/coverage.html,**/logs/**,**/build/resources/**,**/verrazzano_images.txt,**/*cluster-snapshot*/**,**/*bug-report*/**,**/full-cluster/**,**/${TEST_REPORT}", allowEmptyArchive: true
            junit testResults: "**/${TEST_REPORT}", allowEmptyResults: true

            script {
                failBuildIfFailuresDuringArtifactsDump()
            }
        }
        cleanup {
            deleteAll()
        }
    }
}

// false : version parameter falls between vz to(current) and from(to be upgraded) version
// true  : vz to(current) and from(to be upgraded) version falls below version parameter
def checkUpgradeBelowVer(version) {
    if(params.VERSION_FOR_UPGRADE != "NONE" && params.VERSION_FOR_UPGRADE != "current_branch"){
        VERRAZZANO_DEV_VERSION = VERSION_FOR_UPGRADE_ENV
    }
    return !(isMinVersion(params.VERSION_FOR_INSTALL, version) && !isMinVersion(VERRAZZANO_DEV_VERSION, version))
}

// compares the two vz versions v1 & v2
// true : v1 < v2, false : v1 >= v2
def isMinVersion(v1, v2) {
    v2 = v2 - 'v'
    v1 = v1 - 'v'
    def v2Split  = v2.split('\\.')
    runningVZVersion = v2Split[0] + '.' + v2Split[1] + '.' + v2Split[2].charAt(0)
    v1Split = v1.split('\\.')
    if(v1Split[0] < v2Split[0]){
        return true
    }else if(v1Split[0] == v2Split[0] && v1Split[1] < v2Split[1]){
        return true
    }else if(v1Split[0] == v2Split[0] && v1Split[1] == v2Split[1] && v1Split[2].charAt(0) < v2Split[2].charAt(0)){
        return true
    }
    return false
}



def createOCIDNSZone() {
    script {
        dns_zone_ocid = sh(script: "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/oci_dns_ops.sh -o create -c ${TF_VAR_compartment_id} -s z${zoneId}", returnStdout: true)
    }
}

def saveConsoleTestArtifacts() {
    script {
        sh "${GO_REPO_PATH}/verrazzano/ci/scripts/save_console_test_artifacts.sh"
    }
}

def createTestOCIConfigSecret() {
    script {
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                cd ${GO_REPO_PATH}/verrazzano
                ./tests/e2e/config/scripts/create-test-oci-config-secret.sh
            """
        }
    }
}

def deleteAll() {
    // Delete clusters as the very first thing in cleanup to reclaim cluster resources especially OKE resources
    deleteClusters()
    deleteDir()
}

def deleteClusters() {
    script {
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        if (env.TEST_ENV == "KIND") {
            for(int count=1; count<=clusterCount; count++) {
                sh """
                    if [ "${env.TEST_ENV}" == "KIND" ]
                    then
                        kind delete cluster --name ${CLUSTER_NAME_PREFIX}-$count
                    fi
                """
            }
        } else {
            deleteOkeClusters()
        }
    }
}

def copyGeneratedTestReports() {
    script {
        sh """
            # Copy the generated test reports to WORKSPACE to archive them
            mkdir -p ${TEST_REPORT_DIR}
            cd ${GO_REPO_PATH}/verrazzano/tests/e2e
            find . -name "${TEST_REPORT}" | cpio -pdm ${TEST_REPORT_DIR}
        """
    }
}

def failBuildIfFailuresDuringArtifactsDump() {
    script {
        sh """
            if [ -f ${POST_DUMP_FAILED_FILE} ]; then
                echo "Failures seen during dumping of artifacts, treat post as failed"
                exit 1
            fi
        """
    }
}

def printNodeLabels() {
    script {
        sh """
            echo "${NODE_LABELS}"
        """
    }
}

def performNetRCOperations() {
    script {
        sh """
            cp -f "${NETRC_FILE}" $HOME/.netrc
            chmod 600 $HOME/.netrc
        """
    }
}

def performVerrazzanoDirectoryOperations() {
    script {
        sh """
            rm -rf ${GO_REPO_PATH}/verrazzano
            mkdir -p ${GO_REPO_PATH}/verrazzano
            tar cf - . | (cd ${GO_REPO_PATH}/verrazzano/ ; tar xf -)
        """
    }
}

// Utility method to create a map that will be used later to perform parallel operations against multiple clusters
Map createClusterExecutionsMap() {
    script {
        // Create a dictionary of steps to be executed in parallel
        // - the first one will always be the Admin cluster
        // - clusters 2-max are managed clusters
        def clusterExecutionsMap = [:]
        return clusterExecutionsMap
    }
}

// Create a KinD cluster instance
// - count - the cluster index into $KUBECONFIG_DIR
// - metallbAddressRange - the address range to provide the Metallb install within the KinD Docker bridge network address range
// - cleanupKindContainers - indicates to the script whether or not to remove any existing clusters with the same name before creating the new one
// - connectJenkinsRunnerToNetwork - indicates whether or not to connect the KinD Docker bridge network to the Jenkins local docker network
def installKindCluster(count, metallbAddressRange, cleanupKindContainers, connectJenkinsRunnerToNetwork) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return script {
        sh """
                echo ${CLUSTER_NAME_PREFIX}-$count
                echo ${KUBECONFIG_DIR}/$count/kube_config
                mkdir -p ${KUBECONFIG_DIR}/$count
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                echo "Create Kind cluster \$1"
                cd ${TEST_SCRIPTS_DIR}
                # As a stop gap, for now we are using the api/vpo caches here to see if it helps with rate limiting issues, we will need to add specific caches so for now
                # specify the cache name based on the count value, this is assuming 1 or 2 clusters
                case "${count}" in
                    1)
                        ./create_kind_cluster.sh "${CLUSTER_NAME_PREFIX}-$count" "${GO_REPO_PATH}/verrazzano/platform-operator" "${KUBECONFIG_DIR}/$count/kube_config" "${params.KIND_CLUSTER_VERSION}" "$cleanupKindContainers" "$connectJenkinsRunnerToNetwork" true ${params.CREATE_CLUSTER_USE_CALICO} "vpo_integ"
                        ;;
                    2)
                        ./create_kind_cluster.sh "${CLUSTER_NAME_PREFIX}-$count" "${GO_REPO_PATH}/verrazzano/platform-operator" "${KUBECONFIG_DIR}/$count/kube_config" "${params.KIND_CLUSTER_VERSION}" "$cleanupKindContainers" "$connectJenkinsRunnerToNetwork" true ${params.CREATE_CLUSTER_USE_CALICO} "apo_integ"
                        ;;
                    *)
                        ./create_kind_cluster.sh "${CLUSTER_NAME_PREFIX}-$count" "${GO_REPO_PATH}/verrazzano/platform-operator" "${KUBECONFIG_DIR}/$count/kube_config" "${params.KIND_CLUSTER_VERSION}" "$cleanupKindContainers" "$connectJenkinsRunnerToNetwork" false ${params.CREATE_CLUSTER_USE_CALICO} "NONE"
                        ;;
                esac
                if [ ${params.CREATE_CLUSTER_USE_CALICO} == true ]; then
                    echo "Install Calico"
                    cd ${GO_REPO_PATH}/verrazzano
                    ./ci/scripts/install_calico.sh "${CLUSTER_NAME_PREFIX}-$count"
                fi
                kubectl wait --for=condition=ready nodes/${CLUSTER_NAME_PREFIX}-$count-control-plane --timeout=5m
                kubectl wait --for=condition=ready pods/kube-controller-manager-${CLUSTER_NAME_PREFIX}-$count-control-plane -n kube-system --timeout=5m
                echo "Listing pods in kube-system namespace ..."
                kubectl get pods -n kube-system
                echo "Install metallb"
                cd ${GO_REPO_PATH}/verrazzano
                ./tests/e2e/config/scripts/install-metallb.sh $metallbAddressRange
                echo "Deploy external es and create its secret on the admin cluster if EXTERNAL_ELASTICSEARCH is true"
                CLUSTER_NUMBER=${count}  ${CREATE_EXTERNAL_OPENSEARCH}
            """
    }
}

def deleteOkeClusters() {
    script {
        sh """
            mkdir -p ${KUBECONFIG_DIR}
            if [ "${TEST_ENV}" == "ocidns_oke" ]; then
                 cd ${GO_REPO_PATH}/verrazzano
                 ./tests/e2e/config/scripts/oci_dns_ops.sh -o delete -s z${zoneId} || echo "Failed to delete DNS zone z${zoneId}"
             fi
            cd ${TEST_SCRIPTS_DIR}
            TF_VAR_label_prefix=${OKE_CLUSTER_PREFIX} TF_VAR_state_name=multicluster-${env.BUILD_NUMBER}-${env.BRANCH_NAME} ./delete_oke_cluster.sh "$clusterCount" "${KUBECONFIG_DIR}" || true
        """
    }
}

// Either download the specified release of the platform operator YAML, or create one
// using the specific operator image provided by the user.
def getVerrazzanoOperatorYaml() {
    script {
        OPERATOR_YAML_FILE=sh(returnStdout: true, script: "ci/scripts/derive_operator_yaml.sh ${params.VERSION_FOR_INSTALL}").trim()
        RELEASE_NUMBER=sh(returnStdout: true, script: "echo ${VERSION_FOR_INSTALL} | tail -c4").trim()

        sh """
            echo "Platform Operator Configuration"
            cd ${GO_REPO_PATH}/verrazzano
            if [ "NONE" == "${params.VERRAZZANO_OPERATOR_IMAGE}" ]; then
                if [ "master" == $VERSION_FOR_UPGRADE_ENV ]; then
                    echo "Downloading operator from object storage"
                    oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/release-${RELEASE_NUMBER}/${SHORT_COMMIT_HASH}/operator.yaml --file ${WORKSPACE}/downloaded-release-operator.yaml
                else
                    echo "Downloading ${OPERATOR_YAML_FILE} for release ${params.VERSION_FOR_INSTALL}"
                    wget "${OPERATOR_YAML_FILE}" -O "${WORKSPACE}"/downloaded-release-operator.yaml
                fi
                cp ${WORKSPACE}/downloaded-release-operator.yaml ${WORKSPACE}/acceptance-test-operator.yaml
            else
                echo "Generating operator.yaml based on image name provided: ${params.VERRAZZANO_OPERATOR_IMAGE}"
                env IMAGE_PULL_SECRETS=verrazzano-container-registry DOCKER_IMAGE=${params.VERRAZZANO_OPERATOR_IMAGE} ./tools/scripts/generate_operator_yaml.sh > ${WORKSPACE}/acceptance-test-operator.yaml
            fi
        """
    }
}

// Install Verrazzano on each of the clusters
def installVerrazzano() {
    script {
        def verrazzanoInstallStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            def installerPath="${KUBECONFIG_DIR}/${count}/${installerFileName}"
            def key = "vz-mgd-${count-1}"
            if (count == 1) {
                key = "vz-admin"
            }
            verrazzanoInstallStages["${key}"] = installVerrazzanoOnCluster(count, installerPath)
        }
        parallel verrazzanoInstallStages
    }
}

// Install Verrazzano on a target cluster
// - count is the cluster index into the $KUBECONFIG_DIR
// - verrazzanoConfig is the Verrazzano CR to use to install VZ on the cluster
def installVerrazzanoOnCluster(count, verrazzanoConfig) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                cd ${GO_REPO_PATH}/verrazzano

                # Display the kubectl and cluster versions
                kubectl version

                ${LOOPING_TEST_SCRIPTS_DIR}/dump_cluster.sh ${WORKSPACE}/verrazzano/build/resources/cluster${count}/pre-install-resources

                if { [ 'false' == '${params.EXTERNAL_ELASTICSEARCH}' ]; } || { [ '${count}' != 1 ] && [ 'true' == '${params.EXTERNAL_ELASTICSEARCH}' ]; }; then
                    echo "Create the verrazzano-install namespace"
                    kubectl create namespace verrazzano-install
                fi

                echo "Create Image Pull Secrets"
                ./tests/e2e/config/scripts/create-image-pull-secret.sh "${IMAGE_PULL_SECRET}" "${DOCKER_REPO}" "${DOCKER_CREDS_USR}" "${DOCKER_CREDS_PSW}"
                ./tests/e2e/config/scripts/create-image-pull-secret.sh github-packages "${DOCKER_REPO}" "${DOCKER_CREDS_USR}" "${DOCKER_CREDS_PSW}"
                ./tests/e2e/config/scripts/create-image-pull-secret.sh ocr "${OCR_REPO}" "${OCR_CREDS_USR}" "${OCR_CREDS_PSW}"
                ./tests/e2e/config/scripts/create-image-pull-secret.sh "${IMAGE_PULL_SECRET}" "${DOCKER_REPO}" "${DOCKER_CREDS_USR}" "${DOCKER_CREDS_PSW}" "verrazzano-install"

                echo "Following is the verrazzano CR file for the installation:"
                cat ${verrazzanoConfig}

                if [ "${params.INSTALLATION_METHOD_PREUPGRADE}" == "kubectl" ]; then
                    echo "Installing the Verrazzano Platform Operator"
                    kubectl apply -f ${WORKSPACE}/acceptance-test-operator.yaml

                    # create secret in verrazzano-install ns
                    ./tests/e2e/config/scripts/create-image-pull-secret.sh "${IMAGE_PULL_SECRET}" "${DOCKER_REPO}" "${DOCKER_CREDS_USR}" "${DOCKER_CREDS_PSW}" "verrazzano-install"

                    echo "Wait for Operator to be ready"
                    cd ${GO_REPO_PATH}/verrazzano
                    kubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator

                    echo "Applying \$verrazzanoConfig"
                    # validate is set to false since we are using newer Verrazzano CR yaml when installing an older pre-upgrade version
                    kubectl apply --validate=false -f ${verrazzanoConfig}

                    # wait for Verrazzano install to complete
                    ./tests/e2e/config/scripts/wait-for-verrazzano-install.sh
                elif [ "${params.INSTALLATION_METHOD_PREUPGRADE}" == "vz cli" ]; then
                    # Install Verrazzano
                    ${GO_REPO_PATH}/vz install --kubeconfig ${KUBECONFIG_DIR}/${count}/kube_config --filename ${verrazzanoConfig} --manifests ${WORKSPACE}/acceptance-test-operator.yaml
                fi
            """
            RUNNING_OPERATOR_VERSION = sh(returnStdout: true, script: "export KUBECONFIG='${KUBECONFIG_DIR}/${count}/kube_config' && kubectl get verrazzano -o jsonpath='{.items[0].status.version}'").trim()
            echo "Verrazzano Version INSTALl : ${RUNNING_OPERATOR_VERSION}"
        }
    }
}

def getPlatformOperatorForRelease(){
    script {
        //Removing v from the upgrade version for error handling.
        VERSION_FOR_UPGRADE_ENV = VERSION_FOR_UPGRADE_ENV - "v"
        //Command to retrieve the platform operator yaml of the current branch.
        def operatorCMD = "oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/${env.BRANCH_NAME}/${SHORT_COMMIT_HASH}/operator.yaml --file"

        if(params.VERSION_FOR_UPGRADE == "master"){
            operatorCMD = "oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name master/operator.yaml --file"
        
        //Command to retrieve the platform operator yaml of the release branch for patch release upgrade testing.
        }else if(params.VERSION_FOR_UPGRADE != "current_branch" && params.IS_TRIGGERED_MANUALLY != "YES" && params.VERSION_FOR_UPGRADE != "NONE"){

            def latestReleaseBranch = getLatestReleaseVersion()

            operatorCMD = "oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name release-${latestReleaseBranch}/operator.yaml --file"
        //Command to retrieve the released version platform operator yaml.
        }else if(params.IS_TRIGGERED_MANUALLY == "YES" && params.VERSION_FOR_UPGRADE != "NONE" && params.VERSION_FOR_UPGRADE != "current_branch"){
            OPERATOR_YAML_FILE=sh(returnStdout: true, script: "ci/scripts/derive_operator_yaml.sh v${VERSION_FOR_UPGRADE_ENV}").trim()
            operatorCMD = "curl -L ${OPERATOR_YAML_FILE} --output"
        }

        sh(script: "$operatorCMD $TARGET_UPGRADE_OPERATOR_YAML")
    }
}

def getLatestReleaseVersion(){
    def latestReleaseBranch
    if(params.VERSION_FOR_UPGRADE != "NONE" && params.VERSION_FOR_UPGRADE != "current_branch"){
        VERSION_FOR_UPGRADE_ENV = VERSION_FOR_UPGRADE_ENV - "v"
        def latestReleaseTagSplit = VERSION_FOR_UPGRADE_ENV.split("\\.")
        latestReleaseBranch = latestReleaseTagSplit[0] + "." + latestReleaseTagSplit[1]
    }
    return latestReleaseBranch
}

// Upgrade the verrazzano-platform-operator for all clusters
def upgradePlatformOperator() {
    script {
        echo "Downloading target upgrade operator yaml"
        getPlatformOperatorForRelease()

        def verrazzanoPlatformOperatorUpgradeStages= createClusterExecutionsMap()

        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count = 1; count <= clusterCount; count++) {
            def key = "vz-mgd-${count-1}"
            if (count == 1) {
                key = "vz-admin"
            }
            verrazzanoPlatformOperatorUpgradeStages["${key}"] = upgradePlatformOperatorOnCluster(count)
        }
        parallel verrazzanoPlatformOperatorUpgradeStages
    }
}

// Upgrade the verrazzano-platform-operator for a given cluster
// - count is the cluster index into the $KUBECONFIG_DIR
def upgradePlatformOperatorOnCluster(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            def upgradeOperatorFile = "${KUBECONFIG_DIR}/${count}/upgrade-operator.yaml"
            def kubeClusterConfig="${KUBECONFIG_DIR}/${count}/kube_config"
            sh """
                cp ${TARGET_UPGRADE_OPERATOR_YAML} ${upgradeOperatorFile}
                echo "Upgrading the Verrazzano platform operator"
                kubectl --kubeconfig=${kubeClusterConfig} apply -f ${upgradeOperatorFile}

                # need to sleep since the old operator needs to transition to terminating state
                sleep 15

                # ensure operator pod is up
                kubectl --kubeconfig=${kubeClusterConfig} -n verrazzano-install rollout status deployment/verrazzano-platform-operator
                kubectl --kubeconfig=${kubeClusterConfig} -n verrazzano-install rollout status deployment/verrazzano-platform-operator-webhook
            """
        }
    }
}

// Upgrade Verrazzano
def upgradeVerrazzano() {
    script {
        // Download the bom for the target version that we are upgrading to, then extract the version
        // Note, this version will have a semver suffix which is generated for each build, e.g. 1.0.1-33+d592fed6
        if(params.VERSION_FOR_UPGRADE == "master"){
            VERRAZZANO_DEV_VERSION = sh(returnStdout: true, script: "oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name master/generated-verrazzano-bom.json --file - | jq -r '.version'").trim()
        }else if(params.VERSION_FOR_UPGRADE != "NONE" && params.VERSION_FOR_UPGRADE != "current_branch"){
            VERRAZZANO_DEV_VERSION = VERSION_FOR_UPGRADE_ENV
        }else{
            VERRAZZANO_DEV_VERSION = sh(returnStdout: true, script: "oci --region us-phoenix-1 os object get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/${env.BRANCH_NAME}/${SHORT_COMMIT_HASH}/generated-verrazzano-bom.json --file - | jq -r '.version'").trim()
        }

        echo "Downloading target upgrade operator yaml"
        getPlatformOperatorForRelease()

        def verrazzanoUpgradeStages = createClusterExecutionsMap()

        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            def key = "vz-mgd-${count-1}"
            if (count == 1) {
                key = "vz-admin"
            }
            verrazzanoUpgradeStages["${key}"] = upgradeVerrazzanoOnCluster(count, "${VERRAZZANO_DEV_VERSION}")
        }
        parallel verrazzanoUpgradeStages
    }
}

// Upgrade Verrazzano on a target cluster
// - count is the cluster index into the $KUBECONFIG_DIR
// - verrazzanoDevVersion is the Verrazzano version to be upgraded to
def upgradeVerrazzanoOnCluster(count, verrazzanoDevVersion) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            def kubeClusterConfig="${KUBECONFIG_DIR}/${count}/kube_config"
            def upgradeOperatorFile = "${KUBECONFIG_DIR}/${count}/upgrade-operator.yaml"

            def v8oInstallFile="${KUBECONFIG_DIR}/${count}/${installerFileName}"
            def v8oUpgradeFile="${KUBECONFIG_DIR}/${count}/verrazzano-upgrade-cr.yaml"
            sh """
                cp ${TARGET_UPGRADE_OPERATOR_YAML} ${upgradeOperatorFile}

                mkdir -p ${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs/${count}

                # Get the install job in verrazzano-install namespace
                kubectl --kubeconfig=${kubeClusterConfig} -n verrazzano-install get job -o yaml --selector=job-name=verrazzano-install-my-verrazzano > ${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs/${count}/verrazzano-pre-upgrade-job.out

                if [ "${params.INSTALLATION_METHOD_UPGRADE}" == "kubectl" ]; then
                    echo "Upgrading the Verrazzano installation to version" ${verrazzanoDevVersion}
                    # Modify the version field in the Verrazzano CR file to be this new Verrazzano version
                    cd ${GO_REPO_PATH}/verrazzano
                    cp ${v8oInstallFile} ${v8oUpgradeFile}
                    ${TEST_SCRIPTS_DIR}/process_upgrade_yaml.sh  ${verrazzanoDevVersion}  ${v8oUpgradeFile}

                    # Do the upgrade
                    echo "Following is the verrazzano CR file with the new version:"
                    cat ${v8oUpgradeFile}
                    kubectl --kubeconfig=${kubeClusterConfig} patch verrazzano my-verrazzano -p '{"spec":{"version":"'${verrazzanoDevVersion}'"}}' --type=merge
                    # wait for the upgrade to complete
                    kubectl --kubeconfig=${kubeClusterConfig} wait --timeout=35m --for=condition=UpgradeComplete verrazzano/my-verrazzano
                elif [ "${params.INSTALLATION_METHOD_UPGRADE}" == "vz cli" ]; then
                    echo "Upgrading the Verrazzano installation to version" ${verrazzanoDevVersion}
                    ${GO_REPO_PATH}/vz upgrade --kubeconfig ${kubeClusterConfig} --version ${verrazzanoDevVersion} --manifests ${upgradeOperatorFile} --timeout 35m
                fi

                # Get the install job(s) and mke sure the it matches pre-install.  If there is more than 1 job or the job changed, then it won't match
                kubectl --kubeconfig=${kubeClusterConfig} -n verrazzano-install get job -o yaml --selector=job-name=verrazzano-install-my-verrazzano > ${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs/${count}/verrazzano-post-upgrade-job.out

                echo "Ensuring that the install job(s) in verrazzzano-system are identical pre and post install"
                cmp -s ${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs/${count}/verrazzano-pre-upgrade-job.out ${WORKSPACE}/verrazzano/platform-operator/scripts/install/build/logs/${count}/verrazzano-post-upgrade-job.out

                # ideally we don't need to wait here
                sleep 15
                echo "helm list : releases across all namespaces, after upgrading Verrazzano installation ..."
                helm --kubeconfig=${kubeClusterConfig} list -A
            """
            RUNNING_OPERATOR_VERSION = sh(returnStdout: true, script: "export KUBECONFIG='${KUBECONFIG_DIR}/${count}/kube_config' && kubectl get vz -o jsonpath='{.items[0].status.version}'").trim()
            echo "Verrazzano Version Upgrade : ${RUNNING_OPERATOR_VERSION}"
        }
    }
}

// register all managed clusters
def registerManagedClusters() {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            def verrazzanoRegisterManagedClusterStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            for(int count=2; count<=clusterCount; count++) {
                verrazzanoRegisterManagedClusterStages["${count} - Register Managed Cluster"] = registerManagedCluster(count)
            }
            // ADMIN_VZ_VERSION_AT_REGISTRATION is used by verify register test
            env.ADMIN_VZ_VERSION_AT_REGISTRATION = sh(returnStdout: true,
                    script:"KUBECONFIG=${ADMIN_KUBECONFIG} kubectl get verrazzano -o jsonpath='{.items[0].status.version}'").trim()
            print "Admin VZ version at registration is ${env.ADMIN_VZ_VERSION_AT_REGISTRATION}"
            parallel verrazzanoRegisterManagedClusterStages
        }
    }
}

def registerManagedCluster(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export MANAGED_CLUSTER_DIR="${KUBECONFIG_DIR}/${count}"
                export MANAGED_CLUSTER_NAME="managed${count-1}"
                export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export MANAGED_CLUSTER_ENV="mgd${count-1}"
                cd ${GO_REPO_PATH}/verrazzano
                ./tests/e2e/config/scripts/register_managed_cluster.sh
            """
        }
    }
}

// Verify the register of the managed clusters
def verifyRegisterManagedClusters(minimalVerification) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            def verrazzanoRegisterManagedClusterStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            for(int count=2; count<=clusterCount; count++) {
                verrazzanoRegisterManagedClusterStages["${count} - Verify Register Managed Cluster"] = verifyRegisterManagedCluster(count, minimalVerification)
            }
            parallel verrazzanoRegisterManagedClusterStages
        }
    }
}

def verifyRegisterManagedCluster(count, minimalVerification) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export MANAGED_CLUSTER_NAME="managed${count-1}"
                export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo build multicluster/verify-register/
                ginkgo -p --randomize-all -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" multicluster/verify-register/*.test -- --minimalVerification=${minimalVerification}
            """
        }
    }
}

// Verify the deregister of the managed clusters
def verifyDeregisterManagedClusters() {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            def verrazzanoDeregisterManagedClusterStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            for(int count=2; count<=clusterCount; count++) {
                verrazzanoDeregisterManagedClusterStages["${count} - Verify Deregister Managed Cluster"] = verifyDeregisterManagedCluster(count)
            }
            parallel verrazzanoDeregisterManagedClusterStages
        }
    }
}

def verifyDeregisterManagedCluster(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export MANAGED_CLUSTER_NAME="managed${count-1}"
                export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export ADMIN_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
                ./tests/e2e/config/scripts/deregister_managed_cluster.sh
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo build multicluster/verify-deregister/
                ginkgo -p --randomize-all -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" multicluster/verify-deregister/*.test
            """
        }
    }
}

// Verify the managed cluster permissions
def verifyManagedClusterPermissions() {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            def verrazzanoManagedClusterPermissionsStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            for(int count=2; count<=clusterCount; count++) {
                verrazzanoManagedClusterPermissionsStages["${count} - Verify Managed Cluster Permissions"] = verifyManagedClusterPermissions(count)
            }
            parallel verrazzanoManagedClusterPermissionsStages
        }
    }
}

def verifyManagedClusterPermissions(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export MANAGED_CLUSTER_NAME="managed${count-1}"
                export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export MANAGED_ACCESS_KUBECONFIG="${KUBECONFIG_DIR}/${count}/managed_kube_config"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo -v -stream --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" multicluster/verify-permissions/...
            """
        }
    }
}

// Run ginkgo test suites
def runGinkgoVerifyManaged(testSuitePath, clusterDumpDirectory, skipDeploy, skipUndeploy, skipVerify) {
    script {
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        sh """
            export MANAGED_CLUSTER_NAME="managed1"
            export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/2/kube_config"
            export CLUSTER_COUNT=$clusterCount
            export ADMIN_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export CLUSTER_NAME="managed1"
            cd ${GO_REPO_PATH}/verrazzano/tests/e2e
            export DUMP_KUBECONFIG="${KUBECONFIG_DIR}/2/kube_config"
            export DUMP_DIRECTORY="${clusterDumpDirectory}"
            ginkgo -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/... -- --skipDeploy=${skipDeploy} --skipUndeploy=${skipUndeploy} --skipVerify=${skipVerify}
        """
    }
}

// Run a test suite against all clusters in serial
def runGinkgoRandomizeSerial(testSuitePath) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            def clusterName = ""
            for(int count=1; count<=clusterCount; count++) {
                // The first cluster created by this script is named as admin, and the subsequent clusters are named as
                // managed1, managed2, etc.
                if (count == 1) {
                    clusterName="admin"
                } else {
                    clusterName="managed${count-1}"
                }
                sh """
                    export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                    export CLUSTER_NAME="${clusterName}"
                    cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                    ginkgo -p --randomize-all -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/...
                """
            }
        }
    }
}

// Run a test suite against all clusters in parallel
def runGinkgoRandomizeParallel(testSuitePath) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        script {
            def runGinkgoRandomizeStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            def clusterName = ""
            for(int count=1; count<=clusterCount; count++) {
                // The first cluster created by this script is named as admin, and the subsequent clusters are named as
                // managed1, managed2, etc.
                if (count == 1) {
                    clusterName="admin"
                } else {
                    clusterName="managed${count-1}"
                }
                sh """
                    export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                    export CLUSTER_NAME="${clusterName}"
                    cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                    ginkgo build ${testSuitePath}/
                """
                runGinkgoRandomizeStages["${count} - Executing ginkgo test suite ${testSuitePath}"] = runGinkgoRandomize(count, clusterName, testSuitePath)
            }
            parallel runGinkgoRandomizeStages
        }
    }
}

def runGinkgoRandomize(count, clusterName, testSuitePath) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export CLUSTER_NAME="${clusterName}"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo -p --randomize-all -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/*.test
            """
        }
    }
}

// Run a test suite against just the admin cluster
def runGinkgoRandomizeAdmin(testSuitePath, clusterDumpDirectory) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        sh """
            export KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export CLUSTER_NAME="admin"
            export DUMP_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export DUMP_DIRECTORY="${clusterDumpDirectory}"
            cd ${GO_REPO_PATH}/verrazzano/tests/e2e
            ginkgo build ${testSuitePath}/
            ginkgo -p --randomize-all -v --keep-going --no-color -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/*.test
        """
    }
}

// Run a test suite serially against just the admin cluster
def runGinkgoRandomizeSerialAdmin(testSuitePath, clusterDumpDirectory) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        sh """
            export KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export CLUSTER_NAME="admin"
            export DUMP_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export DUMP_DIRECTORY="${clusterDumpDirectory}"
            cd ${GO_REPO_PATH}/verrazzano/tests/e2e
            ginkgo build ${testSuitePath}/
            ginkgo --randomize-all -v --keep-going --no-color -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/*.test
        """
    }
}

def runGinkgoVerify(testSuitePath, clusterDumpDirectory, skipDeploy, skipUndeploy, skipVerify, namespace) {
    catchError(buildResult: 'FAILURE', stageResult: 'FAILURE') {
        sh """
            export KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export CLUSTER_NAME="admin"
            export DUMP_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
            export DUMP_DIRECTORY="${clusterDumpDirectory}"
            cd ${GO_REPO_PATH}/verrazzano/tests/e2e
            ginkgo build ${testSuitePath}/
            ginkgo -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/*.test -- --skipDeploy=${skipDeploy} --skipUndeploy=${skipUndeploy} --skipVerify=${skipVerify} --namespace=${namespace}
        """
    }
}

def runGinkgoVerifyParallel(testSuitePrefix, testSuitePath, clusterDumpDirectory, skipDeploy, skipUndeploy, skipVerify, namespace) {
    script {
        def runGinkgoVerifyStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        def clusterName = ""
        for(int count=1; count<=clusterCount; count++) {
            // The first cluster created by this script is named as admin, and the subsequent clusters are named as
            // managed1, managed2, etc.
            if (count == 1) {
                clusterName="admin"
            } else {
                clusterName="managed${count-1}"
            }
            runGinkgoVerifyStages["${testSuitePrefix} - ${clusterName}"] = runGinkgoVerifyInCluster(count, clusterName, testSuitePath, clusterDumpDirectory, skipDeploy, skipUndeploy, skipVerify, namespace)
        }
        parallel runGinkgoVerifyStages
    }
}

def runGinkgoVerifyInCluster(count, clusterName, testSuitePath, clusterDumpDirectory, skipDeploy, skipUndeploy, skipVerify, namespace) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export CLUSTER_NAME=${clusterName}
                export ADMIN_KUBECONFIG="${KUBECONFIG_DIR}/1/kube_config"
                export DUMP_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                export DUMP_DIRECTORY="${clusterDumpDirectory}${clusterName}"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo build ${testSuitePath}/
                ginkgo -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" ${testSuitePath}/*.test -- --skipDeploy=${skipDeploy} --skipUndeploy=${skipUndeploy} --skipVerify=${skipVerify} --namespace=${namespace}
            """
        }
    }
}

// Configure the Admin and Managed cluster installer custom resources
def configureVerrazzanoInstallers(installResourceTemplate, configProcessorScript, String... extraArgs) {
    script {
        // Concatenate the variable args into a single string
        String allArgs = ""
        extraArgs.each { allArgs += it + " " }

        def verrazzanoInstallerStages = createClusterExecutionsMap()

        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            def destinationPath = "${env.KUBECONFIG_DIR}/${count}/${installerFileName}"
            def installProfile = MANAGED_CLUSTER_PROFILE
            // Installs using OCI DNS require a unique domain per cluster
            // - also, the env name must be <= 10 chars for some reason.
            def envName = "mgd${count-1}"
            if (count == 1) {
                // Cluster "1" is always the admin cluster, use the chosen profile for the VZ install
                // with the env name "admin"
                installProfile = ADMIN_CLUSTER_PROFILE
                envName = "admin"
            }
            verrazzanoInstallerStages["${envName}"] = configureVerrazzanoInstallerOnCluster(count,
                    installResourceTemplate, destinationPath, envName, installProfile, configProcessorScript, allArgs)
        }
        parallel verrazzanoInstallerStages
    }
}

def configureVerrazzanoInstallerOnCluster(count, installResourceTemplate, destinationPath, envName, installProfile,
                                          configProcessorScript, allArgs) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                mkdir -p "${KUBECONFIG_DIR}/${count}"
                export PATH=${HOME}/go/bin:${PATH}
                cd ${GO_REPO_PATH}/verrazzano
                # Copy the template config over for the mgd cluster profile configuration
                cp $installResourceTemplate $destinationPath
                VZ_ENVIRONMENT_NAME="${envName}" INSTALL_PROFILE=$installProfile $configProcessorScript $destinationPath $allArgs
            """
        }
    }
}

// Create the required KinD clusters
def createKindClusters() {
    script {
        sh """
            echo "tests will execute" > ${TESTS_EXECUTED_FILE}
        """
        // NOTE: Eventually we should be able to parallelize the cluster creation, however
        // we seem to be getting some kind of timing issue on cluster create; the 2nd
        // cluster always seems to get a connect/timeout issue, so for now we are keeping
        // the KinD cluster creation serial

        // Can these for now, but eventually we should be able to build this based on
        // inspecting the Docker bridge network CIDR and splitting up a range based on
        // the cluster count.

        def addressRanges = [ "172.18.0.231-172.18.0.238", "172.18.0.239-172.18.0.246", "172.18.0.247-172.18.0.254"]
        def clusterInstallStages = [:]
        boolean cleanupKindContainers = true
        boolean connectJenkinsRunnerToNetwork = true
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            string metallbAddressRange = addressRanges.get(count-1)
            def deployStep = "cluster-${count}"
            // Create dictionary of steps for parallel execution
            //clusterInstallStages[deployStep] = installKindCluster(count, metallbAddressRange, cleanupKindContainers, connectJenkinsRunnerToNetwork)
            // For sequential execution of steps
            installKindCluster(count, metallbAddressRange, cleanupKindContainers, connectJenkinsRunnerToNetwork)
            cleanupKindContainers = false
            connectJenkinsRunnerToNetwork = false
        }
        // Execute steps in parallel
        //parallel clusterInstallStages
    }
}

// Invoke the OKE cluster creation script for the desired number of clusters
def createOKEClusters(clusterPrefix) {
    script {
        sh """
            echo "tests will execute" > ${TESTS_EXECUTED_FILE}
        """
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        sh """
            mkdir -p ${KUBECONFIG_DIR}
            echo "Create OKE cluster"
            cd ${TEST_SCRIPTS_DIR}
            TF_VAR_label_prefix=${clusterPrefix} TF_VAR_state_name=multicluster-${env.BUILD_NUMBER}-${env.BRANCH_NAME} ./create_oke_multi_cluster.sh "$clusterCount" "${KUBECONFIG_DIR}" ${params.CREATE_CLUSTER_USE_CALICO}
        """
    }
}

def dumpAll() {
    dumpVerrazzanoSystemPods()
    dumpCattleSystemPods()
    dumpNginxIngressControllerLogs()
    dumpVerrazzanoPlatformOperatorLogs()
    dumpVerrazzanoApplicationOperatorLogs()
    dumpOamKubernetesRuntimeLogs()
    dumpVerrazzanoApiLogs()
}


def dumpK8sCluster(dumpDirectory) {
    script {
        if ( fileExists(env.TESTS_EXECUTED_FILE) ) {
            def verrazzanoDumpK8sClusterStages = createClusterExecutionsMap()
            int clusterCount = params.TOTAL_CLUSTERS.toInteger()
            for (int count = 1; count <= clusterCount; count++) {
                verrazzanoDumpK8sClusterStages["${count} - Dump K8s Cluster"] = dumpK8sSpecificCluster(count, dumpDirectory)
            }
            parallel verrazzanoDumpK8sClusterStages
        }
    }
}

def dumpK8sSpecificCluster(count, dumpDirectory) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                ${GO_REPO_PATH}/verrazzano/ci/scripts/capture_cluster_snapshot.sh ${dumpDirectory}/cluster-snapshot-${count}
            """
        }
    }
}

def dumpVerrazzanoSystemPods() {
    script {
        def dumpVerrazzanoSystemPodsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpVerrazzanoSystemPodsStages["${count} - Dump Verrazzano System Pods"] = dumpVerrazzanoSystemPods(count)
        }
        parallel dumpVerrazzanoSystemPodsStages
    }
}

def dumpVerrazzanoSystemPods(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${VERRAZZANO_INSTALL_LOGS_DIR}/cluster-$count"
                mkdir -p ${LOG_DIR}
                export DIAGNOSTIC_LOG="${LOG_DIR}/verrazzano-system-pods.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n verrazzano-system -m "verrazzano system pods" || echo "failed" > ${POST_DUMP_FAILED_FILE}
                export DIAGNOSTIC_LOG="${LOG_DIR}/verrazzano-system-certs.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o cert -n verrazzano-system -m "verrazzano system certs" || echo "failed" > ${POST_DUMP_FAILED_FILE}
                export DIAGNOSTIC_LOG="${LOG_DIR}/verrazzano-system-osd.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n verrazzano-system -r "vmi-system-osd-*" -m "verrazzano system opensearchdashboards log" -l -c osd || echo "failed" > ${POST_DUMP_FAILED_FILE}
                export DIAGNOSTIC_LOG="${LOG_DIR}/verrazzano-system-es-master.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n verrazzano-system -r "vmi-system-es-master-*" -m "verrazzano system opensearchdashboards log" -l -c es-master || echo "failed" > ${POST_DUMP_FAILED_FILE}
            """
        }
    }
}

def dumpCattleSystemPods() {
    script {
        def dumpCattleSystemPodsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpCattleSystemPodsStages["${count} - Dump Cattle System Pods"] = dumpCattleSystemPods(count)
        }
        parallel dumpCattleSystemPodsStages
    }
}

def dumpCattleSystemPods(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${VERRAZZANO_INSTALL_LOGS_DIR}/cluster-$count"
                mkdir -p ${LOG_DIR}
                export DIAGNOSTIC_LOG="${LOG_DIR}/cattle-system-pods.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n cattle-system -m "cattle system pods" || echo "failed" > ${POST_DUMP_FAILED_FILE}
                export DIAGNOSTIC_LOG="${LOG_DIR}/rancher.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n cattle-system -r "rancher-*" -m "Rancher logs" -c rancher -l || echo "failed" > ${POST_DUMP_FAILED_FILE}
            """
        }
    }
}

def dumpNginxIngressControllerLogs() {
    script {
        def dumpNginxIngressControllerLogsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpNginxIngressControllerLogsStages["${count} - Dump Nginx Ingress Controller Logs"] = dumpNginxIngressControllerLogs(count)
        }
        parallel dumpNginxIngressControllerLogsStages
    }
}

def dumpNginxIngressControllerLogs(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${VERRAZZANO_INSTALL_LOGS_DIR}/cluster-$count"
                mkdir -p ${LOG_DIR}
                export DIAGNOSTIC_LOG="${LOG_DIR}/nginx-ingress-controller.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n ingress-nginx -r "nginx-ingress-controller-*" -m "Nginx Ingress Controller" -c controller -l || echo "failed" > ${POST_DUMP_FAILED_FILE}
            """
        }
    }
}

def dumpVerrazzanoPlatformOperatorLogs() {
    script {
        def dumpVerrazzanoPlatformOperatorLogsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpVerrazzanoPlatformOperatorLogsStages["${count} - Dump Verrazzano Platform Operator Logs"] = dumpVerrazzanoPlatformOperatorLogs(count)
        }
        parallel dumpVerrazzanoPlatformOperatorLogsStages
    }
}

def dumpVerrazzanoPlatformOperatorLogs(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                ## dump out verrazzano-platform-operator logs
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${WORKSPACE}/verrazzano-platform-operator/logs/cluster-$count"
                mkdir -p ${LOG_DIR}
                kubectl -n verrazzano-install logs --selector=app=verrazzano-platform-operator > ${LOG_DIR}/verrazzano-platform-operator-pod.log --tail -1 || echo "failed" > ${POST_DUMP_FAILED_FILE}
                kubectl -n verrazzano-install describe pod --selector=app=verrazzano-platform-operator > ${LOG_DIR}/verrazzano-platform-operator-pod.out || echo "failed" > ${POST_DUMP_FAILED_FILE}
                echo "verrazzano-platform-operator logs dumped to verrazzano-platform-operator-pod.log"
                echo "verrazzano-platform-operator pod description dumped to verrazzano-platform-operator-pod.out"
                echo "------------------------------------------"
            """
        }
    }
}

def dumpVerrazzanoApplicationOperatorLogs() {
    script {
        def dumpVerrazzanoApplicationOperatorLogsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpVerrazzanoApplicationOperatorLogsStages["${count} - Dump Verrazzano Application Operator Logs"] = dumpVerrazzanoApplicationOperatorLogs(count)
        }
        parallel dumpVerrazzanoApplicationOperatorLogsStages
    }
}

def dumpVerrazzanoApplicationOperatorLogs(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                ## dump out verrazzano-application-operator logs
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${WORKSPACE}/verrazzano-application-operator/logs/cluster-$count"
                mkdir -p ${LOG_DIR}
                kubectl -n verrazzano-system logs --selector=app=verrazzano-application-operator > ${LOG_DIR}/verrazzano-application-operator-pod.log --tail -1 || echo "failed" > ${POST_DUMP_FAILED_FILE}
                kubectl -n verrazzano-system describe pod --selector=app=verrazzano-application-operator > ${LOG_DIR}/verrazzano-application-operator-pod.out || echo "failed" > ${POST_DUMP_FAILED_FILE}
                echo "verrazzano-application-operator logs dumped to verrazzano-application-operator-pod.log"
                echo "verrazzano-application-operator pod description dumped to verrazzano-application-operator-pod.out"
                echo "------------------------------------------"
            """
        }
    }
}

def dumpOamKubernetesRuntimeLogs() {
    script {
        def dumpOamKubernetesRuntimeLogsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpOamKubernetesRuntimeLogsStages["${count} - Dump Oam Kubernetes Runtime Logs"] = dumpOamKubernetesRuntimeLogs(count)
        }
        parallel dumpOamKubernetesRuntimeLogsStages
    }
}

def dumpOamKubernetesRuntimeLogs(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                ## dump out oam-kubernetes-runtime logs
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${WORKSPACE}/oam-kubernetes-runtime/logs/cluster-$count"
                mkdir -p ${LOG_DIR}
                kubectl -n verrazzano-system logs --selector=app.kubernetes.io/instance=oam-kubernetes-runtime > ${LOG_DIR}/oam-kubernetes-runtime-pod.log --tail -1 || echo "failed" > ${POST_DUMP_FAILED_FILE}
                kubectl -n verrazzano-system describe pod --selector=app.kubernetes.io/instance=oam-kubernetes-runtime > ${LOG_DIR}/oam-kubernetes-runtime-pod.out || echo "failed" > ${POST_DUMP_FAILED_FILE}
                echo "verrazzano-application-operator logs dumped to oam-kubernetes-runtime-pod.log"
                echo "verrazzano-application-operator pod description dumped to oam-kubernetes-runtime-pod.out"
                echo "------------------------------------------"
            """
        }
    }
}

def dumpVerrazzanoApiLogs() {
    script {
        def dumpVerrazzanoApiLogsStages = createClusterExecutionsMap()
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            dumpVerrazzanoApiLogsStages["${count} - Dump Verrazzano Api Logs"] = dumpVerrazzanoApiLogs(count)
        }
        parallel dumpVerrazzanoApiLogsStages
    }
}

def dumpVerrazzanoApiLogs(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                export LOG_DIR="${VERRAZZANO_INSTALL_LOGS_DIR}/cluster-$count"
                mkdir -p ${LOG_DIR}
                export DIAGNOSTIC_LOG="${LOG_DIR}/verrazzano-authproxy.log"
                ${GO_REPO_PATH}/verrazzano/platform-operator/scripts/install/k8s-dump-objects.sh -o pods -n verrazzano-system -r "verrazzano-authproxy-*" -m "verrazzano api" -c verrazzano-authproxy -l || echo "failed" > ${POST_DUMP_FAILED_FILE}
            """
        }
    }
}

def dumpInstallLogs() {
    script {
        def verrazzanoDumpInstallLogStages = createClusterExecutionsMap()

        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            LOG_DIR="${VERRAZZANO_INSTALL_LOGS_DIR}/cluster-$count"

            // This function may run on older versions of Verrazzano that have the logs stored in the default namespace
            def namespace = "verrazzano-install"
            if (params.VERSION_FOR_INSTALL.startsWith("v1.0.0") || params.VERSION_FOR_INSTALL.startsWith("v1.0.1")) {
                namespace = "default"
            }
            verrazzanoDumpInstallLogStages["${count} - Dump Install Logs"] = dumpInstallLogsOnCluster(count, LOG_DIR, namespace)
        }
        parallel verrazzanoDumpInstallLogStages
    }
}

def dumpInstallLogsOnCluster(count, logDir, namespace) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                ## dump Verrazzano install logs
                export KUBECONFIG=${KUBECONFIG_DIR}/$count/kube_config
                mkdir -p $logDir
                kubectl -n $namespace logs --selector=job-name=verrazzano-install-my-verrazzano > $logDir/${VERRAZZANO_INSTALL_LOG} --tail -1
                kubectl -n $namespace describe pod --selector=job-name=verrazzano-install-my-verrazzano > $logDir/verrazzano-install-job-pod.out
                echo "------------------------------------------"
            """
        }
    }
}

def getEffectiveDumpOnSuccess() {
    def effectiveValue = params.DUMP_K8S_CLUSTER_ON_SUCCESS
    if (FORCE_DUMP_K8S_CLUSTER_ON_SUCCESS.equals("true") && (env.BRANCH_NAME.equals("master"))) {
        effectiveValue = true
        echo "Forcing dump on success based on global override setting"
    }
    return effectiveValue
}

def setDisplayName() {
    echo "Start setDisplayName"
    def causes = currentBuild.getBuildCauses()
    echo "causes: " + causes.toString()
    for (cause in causes) {
        def causeString = cause.toString()
        echo "current cause: " + causeString
        if (causeString.contains("UpstreamCause") && causeString.contains("Started by upstream project")) {
            echo "This job was caused by " + causeString
            if (causeString.contains("verrazzano-periodic-triggered-tests")) {
                currentBuild.displayName = env.BUILD_NUMBER + " : PERIODIC"
            } else if (causeString.contains("verrazzano-flaky-tests")) {
                currentBuild.displayName = env.BUILD_NUMBER + " : FLAKY"
            }
        }
    }
    echo "End setDisplayName"
}

def modifyHelloHelidonApp(newNamespace, newProjName) {
    sh """
      # create modified versions of the hello helidon MC example
      export MC_HH_DEST_DIR=${GO_REPO_PATH}/verrazzano/examples/multicluster/${newNamespace}
      export MC_HH_SOURCE_DIR=${GO_REPO_PATH}/verrazzano/examples/multicluster/hello-helidon
      export MC_APP_NAMESPACE="${newNamespace}"
      export MC_PROJ_NAME="${newProjName}"
      ${GO_REPO_PATH}/verrazzano/ci/scripts/generate_mc_hello_deployment_files.sh
    """
}

def runMulticlusterVerifyApi() {
    def verrazzanoMulticlusterVerifyApiStages = createClusterExecutionsMap()
    int clusterCount = params.TOTAL_CLUSTERS.toInteger()
    for(int count=2; count<=clusterCount; count++) {
        verrazzanoMulticlusterVerifyApiStages["${count} - Verify Api"] = runMulticlusterVerifyApi(count)
    }
    parallel verrazzanoMulticlusterVerifyApiStages
}

def runMulticlusterVerifyApi(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export MANAGED_CLUSTER_NAME="managed${count-1}"
                export MANAGED_KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" multicluster/verify-api/...
            """
        }
    }
}

def runMulticlusterVerifyRegistry() {
    def verifyRegistryStages = createClusterExecutionsMap()
    int clusterCount = params.TOTAL_CLUSTERS.toInteger()
    for(int count=1; count<=clusterCount; count++) {
        verifyRegistryStages["${count} - Verify registry"] = verifyRegistry(count)
    }
    sh """
        cd ${GO_REPO_PATH}/verrazzano/tests/e2e
        ginkgo build registry/
    """
    parallel verifyRegistryStages
}

def verifyRegistry(count) {
    // For parallel execution, wrap this in a Groovy enclosure {}
    return {
        script {
            sh """
                export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                cd ${GO_REPO_PATH}/verrazzano/tests/e2e
                ginkgo -v --keep-going --no-color ${GINKGO_REPORT_ARGS} -tags="${params.TAGGED_TESTS}" --focus-file="${params.INCLUDED_TESTS}" --skip-file="${params.EXCLUDED_TESTS}" registry/*.test
            """
        }
    }
}

def runConsoleTests() {
    // Set app information used by the application page UI tests to assert for app info
    // Console runs on admin cluster and the KUBECONFIG is pointed at it (which is cluster 1)
    // Make sure that application page tests are also run by setting RUN_APP_TESTS=true since we deployed
    // a sample app for that purpose
    catchError(buildResult: 'SUCCESS', stageResult: 'FAILURE') {
        sh """
            export DUMP_DIRECTORY="${TEST_DUMP_ROOT}/console"
            export CONSOLE_REPO_BRANCH="${params.CONSOLE_REPO_BRANCH}"
            export CONSOLE_APP_NAME="${SAMPLE_APP_NAME}"
            export CONSOLE_APP_NAMESPACE="${SAMPLE_APP_NAMESPACE}"
            export CONSOLE_APP_CLUSTER="managed1"
            export CONSOLE_APP_COMP="${SAMPLE_APP_COMPONENT}"
            KUBECONFIG=${ADMIN_KUBECONFIG} RUN_APP_TESTS=true ${GO_REPO_PATH}/verrazzano/ci/scripts/run_console_tests.sh
        """
    }
}

def setVZConfigForInstallation(){
    if(params.CRD_API_VERSION == "v1beta1"){
        INSTALL_CONFIG_FILE_KIND = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1beta1/install-vz-prod-kind-upgrade.yaml"
        INSTALL_CONFIG_FILE_OCIDNS = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1beta1/install-verrazzano-ocidns.yaml"
        INSTALL_CONFIG_FILE_NIPIO = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1beta1/install-verrazzano-nipio.yaml"
    }
    if(params.VZ_INSTALL_CONFIG == "dev-kind-persistence" && params.CRD_API_VERSION == "v1beta1"){
        INSTALL_CONFIG_FILE_KIND = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1beta1/install-verrazzano-kind-with-persistence.yaml"
        ADMIN_CLUSTER_PROFILE = "dev"
        MANAGED_CLUSTER_PROFILE = "managed-cluster"
    }else if(params.VZ_INSTALL_CONFIG == "dev-kind-persistence" && params.CRD_API_VERSION == "v1alpha1") {
        INSTALL_CONFIG_FILE_KIND = "${GO_REPO_PATH}/verrazzano/tests/e2e/config/scripts/v1alpha1/install-verrazzano-kind-with-persistence.yaml"
        ADMIN_CLUSTER_PROFILE = "dev"
        MANAGED_CLUSTER_PROFILE = "managed-cluster"
    }
}

def isMultiCluster() {
    return params.TOTAL_CLUSTERS.toInteger() >= 2
}

def isVZOperatorMinimumCompatibleVersion(minimumOperatorVersion){

    vzOperatorVersion = RUNNING_OPERATOR_VERSION - 'v'
    minimumOperatorVersion = minimumOperatorVersion - 'v'
    def vzOperatorVersionSplit  = vzOperatorVersion.split('\\.')
    runningVZVersion = vzOperatorVersionSplit[0] + '.' + vzOperatorVersionSplit[1] + '.' + vzOperatorVersionSplit[2].charAt(0)
    echo "Verrazzano Version is: ${runningVZVersion}"
    //Splitting the string minimumOperatorVersion to compare minor and patch versions.
    minimumOperatorVersionSplit = minimumOperatorVersion.split('\\.')

    //Returns true -> when the vzOperatorVersion minor version is greater than the minor version of user defined minimumOperatorVersion
    if(vzOperatorVersionSplit[0] > minimumOperatorVersionSplit[0]){
        return true
    //Returns true -> vzOperatorVersion minor version is greater/equal than the user defined minimumOperatorVersion
    }else if(vzOperatorVersionSplit[0] == minimumOperatorVersionSplit[0] && minimumOperatorVersionSplit[1] < vzOperatorVersionSplit[1]){
              echo "${vzOperatorVersionSplit[0]} >= ${minimumOperatorVersionSplit[0]} && ${minimumOperatorVersionSplit[1]} < ${vzOperatorVersionSplit[1]}"
             return true
    // Returns true ->  when vzOperatorVersion and minimumOperatorVersion has equal minor versions but vzOperatorVersion patch is greater/equal to the user specified minimumOperatorVersion
    }else if(vzOperatorVersionSplit[0] == minimumOperatorVersionSplit[0] && minimumOperatorVersionSplit[1] == vzOperatorVersionSplit[1] && vzOperatorVersionSplit[2].charAt(0) >= minimumOperatorVersionSplit[2].charAt(0)){
        echo "${vzOperatorVersionSplit[0]} == ${minimumOperatorVersionSplit[0]} && ${minimumOperatorVersionSplit[1]} == ${vzOperatorVersionSplit[1]} && ${vzOperatorVersionSplit[2]} >= ${minimumOperatorVersionSplit[2]}"
        return true
    }
    echo "${vzOperatorVersionSplit[0]} > ${minimumOperatorVersionSplit[0]}"
    echo "${vzOperatorVersionSplit[0]} >= ${minimumOperatorVersionSplit[0]} && ${minimumOperatorVersionSplit[1]} < ${vzOperatorVersionSplit[1]}"
    echo "${vzOperatorVersionSplit[0]} == ${minimumOperatorVersionSplit[0]} && ${minimumOperatorVersionSplit[1]} == ${vzOperatorVersionSplit[1]} && ${vzOperatorVersionSplit[2]} >= ${minimumOperatorVersionSplit[2]}"
    echo "Returning False..:("
    return false
}


def downloadCLI() {
    script {
        sh "echo Downloading VZ CLI from object storage"
        if(VERSION_FOR_UPGRADE_ENV == "master"){
            sh "oci --region us-phoenix-1 os object  get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_BUCKET} --name master/${VZ_CLI_TARGZ} --file ${VZ_CLI_TARGZ}"           
        }else{
            sh "oci --region us-phoenix-1 os object  get --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_COMMIT_BUCKET} --name ephemeral/${env.BRANCH_NAME}/${SHORT_COMMIT_HASH}/${VZ_CLI_TARGZ} --file ${VZ_CLI_TARGZ}"
        }
        sh """
            tar xzf ${VZ_CLI_TARGZ} -C ${GO_REPO_PATH}
            ${GO_REPO_PATH}/vz version
        """
    }
}

def setInitValues() {
    script {
        def props = readProperties file: '.verrazzano-development-version'
        VERRAZZANO_DEV_VERSION = props['verrazzano-development-version']
        TIMESTAMP = sh(returnStdout: true, script: "date +%Y%m%d%H%M%S").trim()
        SHORT_COMMIT_HASH = sh(returnStdout: true, script: "git rev-parse --short=8 HEAD").trim()
        // GIT_COMMIT_TO_USE: contains release branch sha which is used to download the release operator for the tip of a release branch.
        // If condition is executed only if the upgrade target branch is master.
        if(params.VERSION_FOR_UPGRADE == 'master'){
            SHORT_COMMIT_HASH = params.GIT_COMMIT_TO_USE[0..7]
        }
        DOCKER_IMAGE_TAG = "${VERRAZZANO_DEV_VERSION}-${TIMESTAMP}-${SHORT_COMMIT_HASH}"
        // update the description with some meaningful info
        setDisplayName()
        currentBuild.description = "from ${params.VERSION_FOR_INSTALL}\n${SHORT_COMMIT_HASH} : ${env.GIT_COMMIT} : ${params.GIT_COMMIT_TO_USE}"
        if (params.TEST_ENV != "KIND") {
            // derive the prefix for the OKE cluster
            OKE_CLUSTER_PREFIX = sh(returnStdout: true, script: "${GO_REPO_PATH}/verrazzano/ci/scripts/derive_oke_cluster_name.sh").trim()
        }
        // Sets the VZ installation configuration based on VZ_INSTALL_CONFIG parameter.
        setVZConfigForInstallation()
    }
}

def performDockerLogin() {
    script {
        try {
            sh """
                echo "${DOCKER_CREDS_PSW}" | docker login ${env.DOCKER_REPO} -u ${DOCKER_CREDS_USR} --password-stdin
            """
        } catch(error) {
            echo "docker login failed, retrying after sleep"
            retry(4) {
                sleep(30)
                sh """
                    echo "${DOCKER_CREDS_PSW}" | docker login ${env.DOCKER_REPO} -u ${DOCKER_CREDS_USR} --password-stdin
                """
            }
        }
    }
}

def performSourceCodeCheckout() {
    script {
        EFFECTIVE_DUMP_K8S_CLUSTER_ON_SUCCESS = getEffectiveDumpOnSuccess()
        if (params.GIT_COMMIT_TO_USE == "NONE") {
            echo "Specific GIT commit was not specified, use current head"
            def scmInfo = checkout scm
            env.GIT_COMMIT = scmInfo.GIT_COMMIT
            env.GIT_BRANCH = scmInfo.GIT_BRANCH
        // The master branch is checked out specifically when the upgrade target branch is master. 
        // This is done to ensure the master branch upgrade test suites are run after the upgrade is completed.
        // Since the target upgrade branch is the master, then the tests run after upgrade must also be from the master branch.
        // This is here to ensure that happens.
        }else if(params.VERSION_FOR_UPGRADE == 'master'){
            def scmInfo = checkout([
                    $class: 'GitSCM',
                    branches: [[name: 'master']],
                    doGenerateSubmoduleConfigurations: false,
                    extensions: [],
                    submoduleCfg: [],
                    userRemoteConfigs: [[url: env.SCM_VERRAZZANO_GIT_URL]]])
            env.GIT_COMMIT = scmInfo.GIT_COMMIT
            env.GIT_BRANCH = scmInfo.GIT_BRANCH
        }else {
            echo "SCM checkout of ${params.GIT_COMMIT_TO_USE}"
            def scmInfo = checkout([
                    $class: 'GitSCM',
                    branches: [[name: params.GIT_COMMIT_TO_USE]],
                    doGenerateSubmoduleConfigurations: false,
                    extensions: [],
                    submoduleCfg: [],
                    userRemoteConfigs: [[url: env.SCM_VERRAZZANO_GIT_URL]]])
            env.GIT_COMMIT = scmInfo.GIT_COMMIT
            env.GIT_BRANCH = scmInfo.GIT_BRANCH
            // If the commit we were handed is not what the SCM says we are using, fail
            if (!env.GIT_COMMIT.equals(params.GIT_COMMIT_TO_USE)) {
                echo "SCM didn't checkout the commit we expected. Expected: ${params.GIT_COMMIT_TO_USE}, Found: ${scmInfo.GIT_COMMIT}"
                exit 1
            }
        }
        echo "SCM checkout of ${env.GIT_BRANCH} at ${env.GIT_COMMIT}"
    }
}

def pipelinePostFailure() {
    script {
        sh """
            curl -k -u ${JENKINS_READ_USR}:${JENKINS_READ_PSW} -o ${WORKSPACE}/build-console-output.log ${BUILD_URL}consoleText
        """
        archiveArtifacts artifacts: '**/build-console-output.log,**/Screenshot*.png,**/ConsoleLog*.log', allowEmptyArchive: true
        // Ignore failures in any of the following actions so that the "always" post step that cleans up clusters is executed
        sh """
            curl -k -u ${JENKINS_READ_USR}:${JENKINS_READ_PSW} -o archive.zip ${BUILD_URL}artifact/*zip*/archive.zip || true
            oci --region us-phoenix-1 os object put --force --namespace ${OCI_OS_NAMESPACE} -bn ${OCI_OS_ARTIFACT_BUCKET} --name ${env.JOB_NAME}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}/archive.zip --file archive.zip || true
            rm archive.zip || true
        """
    }
}

def performDescribeVerrazzanoResource() {
    script {
        int clusterCount = params.TOTAL_CLUSTERS.toInteger()
        for(int count=1; count<=clusterCount; count++) {
            sh """
                echo "Performing describe on Verrazzano resource on cluster ${count}"
                export KUBECONFIG="${KUBECONFIG_DIR}/${count}/kube_config"
                kubectl get vz -o yaml
            """
        }
    }
}
